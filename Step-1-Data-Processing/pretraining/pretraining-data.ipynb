{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued-Pretraining\n",
    "\n",
    "we just concatenate all rows for each item in every column to build the pretraining set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./processed/pretrain.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../raw-data/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate the desired JSONL format\n",
    "def generate_jsonl_format(df):\n",
    "    jsonl_data = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column E (index 4)\n",
    "        product = f\"Simpler Trading Product: {df.iloc[3, col]}\"  # E4 (index 3 - 1)\n",
    "        body = (\n",
    "            f\"{df.iloc[3, col]}\\n\\n\"  # E4\n",
    "            f\"By: \\n{df.iloc[4, col]}\\n\\n\"  # E5\n",
    "            f\"Available on: \\n{df.iloc[5, col]}\\n\\n\"  # E6\n",
    "            f\"This product is designed for: \\n{df.iloc[6, col]}\\n\\n\"  # E7\n",
    "            f\"{df.iloc[7, col]}\\n\\n\"  # E8\n",
    "            f\"{df.iloc[11, col]}\\n\\n\"  # E12\n",
    "            f\"{df.iloc[12, col]}\\n\\n\"  # E13\n",
    "            f\"{df.iloc[13, col]}\\n\\n\"  # E14\n",
    "            f\"{df.iloc[14, col]}\\n\\n\"  # E15\n",
    "            f\"{df.iloc[15, col]}\\n\\n\"  # E16\n",
    "            f\"{df.iloc[16, col]}\\n\\n\"  # E17\n",
    "            f\"{df.iloc[17, col]}\\n\\n\"  # E18\n",
    "            f\"{df.iloc[18, col]}\\n\\n\"  # E19\n",
    "            f\"{df.iloc[19, col]}\\n\\n\"  # E20\n",
    "            f\"{df.iloc[20, col]}\\n\\n\"  # E21\n",
    "            f\"{df.iloc[21, col]}\\n\\n\"  # E22\n",
    "            f\"{df.iloc[22, col]}\\n\\n\"  # E23\n",
    "            f\"{df.iloc[23, col]}\\n\\n\"  # E24\n",
    "            f\"{df.iloc[24, col]}\\n\\n\"  # E25\n",
    "            f\"{df.iloc[25, col]}\\n\\n\"  # E26\n",
    "            f\"{df.iloc[26, col]}\\n\\n\"  # E27\n",
    "            f\"{df.iloc[27, col]}\\n\\n\"  # E28\n",
    "            f\"{df.iloc[28, col]}\\n\\n\"  # E29\n",
    "            f\"{df.iloc[29, col]}\\n\\n\"  # E30\n",
    "            f\"{df.iloc[30, col]}\\n\\n\"  # E31\n",
    "            f\"{df.iloc[31, col]}\\n\\n\"  # E32\n",
    "            f\"{df.iloc[32, col]}\\n\\n\"  # E33\n",
    "            f\"{df.iloc[33, col]}\\n\\n\"  # E34\n",
    "            f\"{df.iloc[34, col]}\\n\\n\"  # E35\n",
    "            f\"{df.iloc[35, col]}\\n\\n\"  # E36\n",
    "            f\"{df.iloc[36, col]}\\n\\n\"  # E37\n",
    "            f\"{df.iloc[37, col]}\\n\\n\"  # E38\n",
    "            f\"{df.iloc[38, col]}\\n\\n\"  # E39\n",
    "            f\"{df.iloc[3, col]} Testimonials: \\n\\n\"  # E4\n",
    "            f\"{df.iloc[39, col]}\\n\\n\"  # E40\n",
    "            f\"{df.iloc[40, col]}\\n\\n\"  # E41\n",
    "            f\"{df.iloc[41, col]}\\n\\n\"  # E42\n",
    "            f\"{df.iloc[42, col]}\\n\\n\"  # E43\n",
    "            f\"{df.iloc[43, col]}\\n\\n\"  # E44\n",
    "            f\"{df.iloc[44, col]}\\n\\n\"  # E45\n",
    "            f\"{df.iloc[45, col]}\\n\\n\"  # E46\n",
    "            f\"{df.iloc[46, col]}\\n\\n\"  # E47\n",
    "            f\"{df.iloc[47, col]}\\n\\n\"  # E48\n",
    "            f\"{df.iloc[48, col]}\\n\\n\"  # E49\n",
    "            f\"{df.iloc[49, col]}\\n\\n\"  # E50\n",
    "            f\"{df.iloc[40, col]}\\n\\n\"  # E51\n",
    "            f\"{df.iloc[3, col]} Frequently Asked Questions: \\n\\n\"  # E4\n",
    "            f\"{df.iloc[51, col]}\\n\\n\"  # E52\n",
    "            f\"{df.iloc[52, col]}\\n\\n\"  # E53\n",
    "            f\"{df.iloc[53, col]}\\n\\n\"  # E54\n",
    "            f\"{df.iloc[54, col]}\\n\\n\"  # E55\n",
    "            f\"{df.iloc[55, col]}\\n\\n\"  # E56\n",
    "            f\"{df.iloc[56, col]}\\n\\n\"  # E57\n",
    "            f\"{df.iloc[57, col]}\\n\\n\"  # E58\n",
    "            f\"{df.iloc[58, col]}\\n\\n\"  # E59\n",
    "        )\n",
    "\n",
    "        jsonl_data.append({\"product\": product, \"body\": body})\n",
    "    \n",
    "    return jsonl_data\n",
    "\n",
    "# Generate the JSONL data\n",
    "jsonl_data = generate_jsonl_format(df)\n",
    "\n",
    "# Save the data to a JSONL file\n",
    "output_file_path = './processed/pretrain.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for item in jsonl_data:\n",
    "        json.dump(item, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "print(output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean miscellaneous nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved as ./cleaned/cleaned_pretrain.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./cleaned/cleaned_pretrain.jsonl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_nan_in_body(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            # Parse the JSON object\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Clean the 'body' key if it exists\n",
    "            if 'body' in data:\n",
    "                body = data['body']\n",
    "                # Remove \"\\nnan\\n\" first, then \"nan\"\n",
    "                body = body.replace(\"\\nnan\\n\", \"\").replace(\"nan\", \"\")\n",
    "                data['body'] = body\n",
    "            \n",
    "            # Write the cleaned JSON object back to the output file\n",
    "            outfile.write(json.dumps(data) + '\\n')\n",
    "    \n",
    "    print(f\"Cleaned file saved as {output_file}\")\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = './processed/pretrain.jsonl'\n",
    "output_file_path = './cleaned/cleaned_pretrain.jsonl'\n",
    "\n",
    "# Run the cleanup function\n",
    "clean_nan_in_body(input_file_path, output_file_path)\n",
    "\n",
    "# Output the path of the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine `max_seq_length` set a cutoff to save memory we use 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
    "max_seq_length = 512 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.754 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_sequences(text, max_length=max_seq_length):\n",
    "    input_ids = tokenizer.encode_plus(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(input_ids) <= max_length:\n",
    "        return [text]\n",
    "\n",
    "    parts = []\n",
    "    part_number = 1\n",
    "    title_end_index = text.find('.  ### Article:')\n",
    "\n",
    "    if title_end_index == -1:\n",
    "        header = text[:max_length]\n",
    "        content = text[max_length:]\n",
    "    else:\n",
    "        header = text[:title_end_index + len('.  ### Article:')]\n",
    "        content = text[title_end_index + len('.  ### Article:'):]\n",
    "\n",
    "    for i in range(0, len(input_ids), max_length):\n",
    "        part_ids = input_ids[i:i + max_length]\n",
    "        part_text = tokenizer.decode(part_ids, skip_special_tokens=True)\n",
    "\n",
    "        if part_number > 1:\n",
    "            part_text = f\"{header} Part {part_number}.\\n\\n{part_text[len(header):]}\"\n",
    "        else:\n",
    "            part_text = f\"{header}{part_text[len(header):]}\"\n",
    "\n",
    "        parts.append(part_text)\n",
    "        part_number += 1\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label so the llm can make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file_path = \"./cleaned/cleaned_pretrain.jsonl\"\n",
    "output_file_path = \"split-pretrain.jsonl\"\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        entry = json.loads(line)\n",
    "        product = entry['product']\n",
    "        body = entry['body']\n",
    "\n",
    "        text = f\"### Title: {product}.  ### Article: {body}\"\n",
    "        split_texts = split_long_sequences(text, max_length=max_seq_length)\n",
    "\n",
    "        for i, split_text in enumerate(split_texts):\n",
    "            if i > 0:\n",
    "                split_text = split_text.replace(f\"### Title: {product}.\", f\"### Title: {product} Part {i + 1}.\")\n",
    "            new_entry = {\"text\": split_text}\n",
    "            outfile.write(json.dumps(new_entry) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze to see what the `max_seq_lengths` are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_sequence_lengths(file_path):\n",
    "    max_seq_length = 0\n",
    "    total_sequences = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            text = entry['text']\n",
    "            tokens = tokenizer.encode_plus(text, add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "            max_seq_length = max(max_seq_length, len(tokens))\n",
    "            total_sequences += 1\n",
    "\n",
    "    buffer = 10\n",
    "    max_seq_length += buffer\n",
    "\n",
    "    print(f\"\\nAnalysis for {file_path}\")\n",
    "    print(f\"{'Measure':<20}{'Value':<14}\")\n",
    "    print(f\"{'-'*34}\")\n",
    "    print(f\"{'Max Seq Length':<20}{max_seq_length:<14}\")\n",
    "    print(f\"{'Total Sequences':<20}{total_sequences:<14}\")\n",
    "    print(f\"\\nRecommendation: Set max_seq_length to at least {max_seq_length} to handle the maximum number of tokens in this dataset (Maximum + Buffer).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Dataset Analysis:\n",
      "\n",
      "Analysis for split-pretrain.jsonl\n",
      "Measure             Value         \n",
      "----------------------------------\n",
      "Max Seq Length      542           \n",
      "Total Sequences     184           \n",
      "\n",
      "Recommendation: Set max_seq_length to at least 542 to handle the maximum number of tokens in this dataset (Maximum + Buffer).\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCombined Dataset Analysis:\")\n",
    "calculate_max_sequence_lengths(output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "premac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
