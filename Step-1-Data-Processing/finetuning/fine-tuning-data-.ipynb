{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What platform can {product} be used with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_jsonl(input_csv, output_jsonl):\n",
    "    def format_platforms(platforms):\n",
    "        platforms_list = platforms.split('|')\n",
    "        if len(platforms_list) == 1:\n",
    "            return platforms\n",
    "        elif len(platforms_list) == 2:\n",
    "            return ' and '.join(platforms_list)\n",
    "        else:\n",
    "            return ', '.join(platforms_list[:-1]) + ', and ' + platforms_list[-1]\n",
    "\n",
    "    print(f\"Opening CSV file: {input_csv}\")\n",
    "    with open(input_csv, 'r', encoding='utf-8') as csv_file, open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        print(\"Skipping first 3 rows...\")\n",
    "        next(csv_reader)  # Skip the first 3 rows\n",
    "        next(csv_reader)\n",
    "        next(csv_reader)\n",
    "        product_names = next(csv_reader)[4:]  # Get product names from row 4, starting from column E\n",
    "        print(f\"Found {len(product_names)} product names: {product_names[:5]}...\")\n",
    "\n",
    "        print(\"Searching for the platforms row...\")\n",
    "        for row_num, row in enumerate(csv_reader, start=5):\n",
    "            print(f\"Checking row {row_num}: {row[:5]}...\")\n",
    "            if row and len(row) > 2 and \"What platforms can\" in row[2]:\n",
    "                print(f\"Found platforms row: {row[:5]}...\")\n",
    "                for i, product in enumerate(product_names):\n",
    "                    if i + 4 < len(row) and row[i+4]:  # Check if there's a value for this product\n",
    "                        question = f\"What platforms can {product} be used on?\"\n",
    "                        answer = f\"{product} can be used on {format_platforms(row[i+4])}\"\n",
    "                        json_line = json.dumps({\"question\": question, \"answer\": answer})\n",
    "                        jsonl_file.write(json_line + '\\n')\n",
    "                        print(f\"Wrote entry for {product}\")\n",
    "                print(\"Finished processing platforms row\")\n",
    "                break  # We've found the row we need, no need to continue\n",
    "        else:\n",
    "            print(\"WARNING: Did not find a row containing platform information!\")\n",
    "\n",
    "    print(\"JSONL file creation process completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_csv_to_jsonl('/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv', 'products2.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_csv_to_jsonl(input_csv, output_jsonl):\n",
    "    def format_platforms(platforms):\n",
    "        if not platforms or platforms == \"N/A\":\n",
    "            return platforms\n",
    "        platforms_list = [p.strip() for p in platforms.split('|')]\n",
    "        if len(platforms_list) == 1:\n",
    "            return platforms\n",
    "        elif len(platforms_list) == 2:\n",
    "            return ' and '.join(platforms_list)\n",
    "        else:\n",
    "            return ', '.join(platforms_list[:-1]) + ', and ' + platforms_list[-1]\n",
    "\n",
    "    question_templates = [\n",
    "        \"What platforms can the {product} be used on?\",\n",
    "        \"On which platforms can {product} be used?\",\n",
    "        \"Where is {product} available for use?\",\n",
    "        \"If I want to use {product}, which platforms should I have access to?\",\n",
    "        \"What platform features are required to run {product}?\",\n",
    "        \"How does the platform compatibility of {product} compare to other products?\",\n",
    "        \"Are there any major platforms that don't support {product}?\",\n",
    "        \"Which platforms are compatible with {product}?\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Opening CSV file: {input_csv}\")\n",
    "    with open(input_csv, 'r', encoding='utf-8') as csv_file, open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        rows = list(csv_reader)\n",
    "\n",
    "        print(\"Processing product names...\")\n",
    "        product_names = [name for name in rows[3][4:] if name]\n",
    "        print(f\"Found {len(product_names)} product names.\")\n",
    "\n",
    "        print(\"Searching for the platforms row...\")\n",
    "        for row in rows:\n",
    "            if row and len(row) > 2 and \"What platforms can the\" in row[2]:\n",
    "                print(\"Found platforms row.\")\n",
    "                for i, product in enumerate(product_names):\n",
    "                    if i + 4 < len(row) and row[i+4]:\n",
    "                        platforms = format_platforms(row[i+4])\n",
    "                        for template in question_templates:\n",
    "                            question = template.format(product=product)\n",
    "                            answer = f\"{product} can be used on {platforms}\"\n",
    "                            json_line = json.dumps({\"question\": question, \"answer\": answer})\n",
    "                            jsonl_file.write(json_line + '\\n')\n",
    "                        print(f\"Wrote entries for {product}\")\n",
    "                print(\"Finished processing platforms row\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"WARNING: Did not find a row containing platform information!\")\n",
    "\n",
    "    print(\"JSONL file creation process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening CSV file: /home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv\n",
      "Processing product names...\n",
      "Found 68 product names.\n",
      "Searching for the platforms row...\n",
      "Found platforms row.\n",
      "Wrote entries for John Carter's Sandbox Strategy\n",
      "Wrote entries for Top Tier Pro System\n",
      "Wrote entries for The PMZ System\n",
      "Wrote entries for The New “Big 3” Squeeze Master Class\n",
      "Wrote entries for Day Trading with Tr3ndy Zones\n",
      "Wrote entries for Micro Voodoo Line Strategy\n",
      "Wrote entries for DPMR Masterclass\n",
      "Wrote entries for Chart Patterns Secrets\n",
      "Wrote entries for The Stacked Profit Strategy\n",
      "Wrote entries for The Multi Squeeze Pro System\n",
      "Wrote entries for Raghee's Day Trading Options Strategy Course\n",
      "Wrote entries for Tr3ndy Jon’s Supply\n",
      "& Demand System\n",
      "Wrote entries for The Ready. Aim. Fire!® Pro System\n",
      "\n",
      "Wrote entries for True Momentum System\n",
      "Wrote entries for Bulletproof Butterflies 2.0\n",
      "Wrote entries for Decoding Volume\n",
      "Wrote entries for 5 Star Options Income Plan\n",
      "Wrote entries for The Moxie Indicator\n",
      "Wrote entries for Strike Zone Strategy\n",
      "Wrote entries for Options Freedom Formula\n",
      "Wrote entries for Small Account Futures\n",
      "Wrote entries for Small Account Blueprint\n",
      "Wrote entries for Quarterly Profits Formula\n",
      "Wrote entries for Ultimate Spread Strategy\n",
      "Wrote entries for Next Level Futures\n",
      "Wrote entries for Volume Breakout System\n",
      "Wrote entries for True Low Formula\n",
      "Wrote entries for TG's $100K Day Setup\n",
      "Wrote entries for Phoenix Finder Turbo\n",
      "Wrote entries for The Obnoxious Profit Strategy\n",
      "Wrote entries for New Multi 10X on Steroids\n",
      "Wrote entries for John Carter's Squeeze Pro System\n",
      "Wrote entries for Ready.Aim.Fire!® (RAF) Pro Indicator\n",
      "Wrote entries for John Carter's Squeeze Pro Indicator\n",
      "Wrote entries for Raghee Horner’s DPMR Indicator\n",
      "Wrote entries for Tr3ndy Script Bundle\n",
      "Wrote entries for Raghee Horner’s HPMR Indicator\n",
      "Wrote entries for TG's Moxie Indicator + Scans\n",
      "Wrote entries for Quant Pivots\n",
      "Wrote entries for Darvas Box 3.0\n",
      "Wrote entries for True Low Indicator\n",
      "Wrote entries for Trend Spark Indicator\n",
      "Wrote entries for Volume Max Tool Kit (formerly VWAP)\n",
      "Wrote entries for Phoenix Finder\n",
      "Wrote entries for Turbo VZO\n",
      "Wrote entries for Divergent Bar\n",
      "Wrote entries for Compound Breakout Tool\n",
      "Wrote entries for Voodoo Lines®\n",
      "Wrote entries for Multi Squeeze Pro \n",
      "Wrote entries for Volume Max Tool Package \n",
      "Wrote entries for Earnings Hot Zone \n",
      "Wrote entries for 10x Bars Indicator \n",
      "Wrote entries for Drama Free Day Trades\n",
      "Wrote entries for DynaRange Indicator \n",
      "Wrote entries for Simpler Options 101\n",
      "Wrote entries for John Carter’s Trend Rotation Tool\n",
      "Wrote entries for Early In N Out Pro\n",
      "Wrote entries for Multi 10x Indicator\n",
      "Wrote entries for Micro Voodoo Lines Indicator \n",
      "Wrote entries for Squeeze Rotation Tool \n",
      "Wrote entries for Squeeze Pro Stats Tool \n",
      "Wrote entries for Compound Growth Mastery Trial \n",
      "Wrote entries for The Overnight Earnings Method\n",
      "Wrote entries for MEM Edge Report \n",
      "Finished processing platforms row\n",
      "JSONL file creation process completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "process_csv_to_jsonl('/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv', 'multiqproducts.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening CSV file: /home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv\n",
      "Processing product names...\n",
      "Found 68 product names.\n",
      "Processing asset classes...\n",
      "Processing John Carter's Sandbox Strategy: Asset classes = Options\n",
      "Wrote entries for John Carter's Sandbox Strategy\n",
      "Processing Top Tier Pro System: Asset classes = All asset classes but most trading done in Options\n",
      "Wrote entries for Top Tier Pro System\n",
      "Processing The PMZ System: Asset classes = Options, Futures, Stocks, and 0DTE\n",
      "Wrote entries for The PMZ System\n",
      "Processing The New “Big 3” Squeeze Master Class: Asset classes = Options, Stocks, and Futures\n",
      "Wrote entries for The New “Big 3” Squeeze Master Class\n",
      "Processing Day Trading with Tr3ndy Zones: Asset classes = 0DTE, Options, Futures, and Stocks\n",
      "Wrote entries for Day Trading with Tr3ndy Zones\n",
      "Processing Micro Voodoo Line Strategy: Asset classes = Options and Futures\n",
      "Wrote entries for Micro Voodoo Line Strategy\n",
      "Processing DPMR Masterclass: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for DPMR Masterclass\n",
      "Processing Chart Patterns Secrets: Asset classes = Stocks, Options, and Futures\n",
      "Wrote entries for Chart Patterns Secrets\n",
      "Processing The Stacked Profit Strategy: Asset classes = Options\n",
      "Wrote entries for The Stacked Profit Strategy\n",
      "Processing The Multi Squeeze Pro System: Asset classes = Options\n",
      "Wrote entries for The Multi Squeeze Pro System\n",
      "Processing Raghee's Day Trading Options Strategy Course: Asset classes = Options\n",
      "Wrote entries for Raghee's Day Trading Options Strategy Course\n",
      "Processing Tr3ndy Jon’s Supply\n",
      "& Demand System: Asset classes = Options, Futures, Stocks, and 0DTE\n",
      "Wrote entries for Tr3ndy Jon’s Supply\n",
      "& Demand System\n",
      "Processing The Ready. Aim. Fire!® Pro System\n",
      ": Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for The Ready. Aim. Fire!® Pro System\n",
      "\n",
      "Processing True Momentum System: Asset classes = Options, Stocks, Futures, and ETFs\n",
      "Wrote entries for True Momentum System\n",
      "Processing Bulletproof Butterflies 2.0: Asset classes = Options and Stocks\n",
      "Wrote entries for Bulletproof Butterflies 2.0\n",
      "Processing Decoding Volume: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Decoding Volume\n",
      "Processing 5 Star Options Income Plan: Asset classes = Options\n",
      "Wrote entries for 5 Star Options Income Plan\n",
      "Processing The Compound Butterfly Blueprint: Asset classes = Options\n",
      "Wrote entries for The Compound Butterfly Blueprint\n",
      "Processing Pocketing Premium Masterclass: Asset classes = Options\n",
      "Wrote entries for Pocketing Premium Masterclass\n",
      "Processing The Moxie Indicator: Asset classes = Stock, Options, Futures, and 0DTE\n",
      "Wrote entries for The Moxie Indicator\n",
      "Processing Strike Zone Strategy: Asset classes = Futures\n",
      "Wrote entries for Strike Zone Strategy\n",
      "Processing Options Freedom Formula: Asset classes = Options\n",
      "Wrote entries for Options Freedom Formula\n",
      "Processing Small Account Futures: Asset classes = Futures\n",
      "Wrote entries for Small Account Futures\n",
      "Processing Small Account Blueprint: Asset classes = Options\n",
      "Wrote entries for Small Account Blueprint\n",
      "Processing Quarterly Profits Formula: Asset classes = Options\n",
      "Wrote entries for Quarterly Profits Formula\n",
      "Processing Ultimate Spread Strategy: Asset classes = Stocks \n",
      "Wrote entries for Ultimate Spread Strategy\n",
      "Processing Next Level Futures: Asset classes = Options, Stocks, Indexes, ETFs, and and Futures\n",
      "Wrote entries for Next Level Futures\n",
      "Processing Volume Breakout System: Asset classes = Futures\n",
      "Wrote entries for Volume Breakout System\n",
      "Processing True Low Formula: Asset classes = Futures\n",
      "Wrote entries for True Low Formula\n",
      "Processing TG's $100K Day Setup: Asset classes = Options\n",
      "Wrote entries for TG's $100K Day Setup\n",
      "Processing Phoenix Finder Turbo: Asset classes = Stocks\n",
      "Wrote entries for Phoenix Finder Turbo\n",
      "Processing The Obnoxious Profit Strategy: Asset classes = Futures, Options, and Stocks\n",
      "Wrote entries for The Obnoxious Profit Strategy\n",
      "Warning: No asset class information for Simpler Calendars\n",
      "Processing New Multi 10X on Steroids: Asset classes = Futures/Options/ETFs\n",
      "Wrote entries for New Multi 10X on Steroids\n",
      "Processing John Carter's Squeeze Pro System: Asset classes = Options\n",
      "Wrote entries for John Carter's Squeeze Pro System\n",
      "Processing Ready.Aim.Fire!® (RAF) Pro Indicator: Asset classes = Options\n",
      "Wrote entries for Ready.Aim.Fire!® (RAF) Pro Indicator\n",
      "Processing John Carter's Squeeze Pro Indicator: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for John Carter's Squeeze Pro Indicator\n",
      "Processing Raghee Horner’s DPMR Indicator: Asset classes = Options\n",
      "Wrote entries for Raghee Horner’s DPMR Indicator\n",
      "Processing Tr3ndy Script Bundle: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Tr3ndy Script Bundle\n",
      "Processing Raghee Horner’s HPMR Indicator: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for Raghee Horner’s HPMR Indicator\n",
      "Processing TG's Moxie Indicator + Scans: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for TG's Moxie Indicator + Scans\n",
      "Processing Quant Pivots: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for Quant Pivots\n",
      "Processing Darvas Box 3.0: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Darvas Box 3.0\n",
      "Processing True Low Indicator: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for True Low Indicator\n",
      "Processing Trend Spark Indicator: Asset classes = Futures, Options, and Stocks\n",
      "Wrote entries for Trend Spark Indicator\n",
      "Processing Volume Max Tool Kit (formerly VWAP): Asset classes = Stocks, Options, Futures, and ETFs\n",
      "Wrote entries for Volume Max Tool Kit (formerly VWAP)\n",
      "Processing Phoenix Finder: Asset classes = Futures\n",
      "Wrote entries for Phoenix Finder\n",
      "Processing Turbo VZO: Asset classes = Futures, ETF's, Stocks, and Options\n",
      "Wrote entries for Turbo VZO\n",
      "Processing Divergent Bar: Asset classes = Stocks, Options, and ETFs\n",
      "Wrote entries for Divergent Bar\n",
      "Processing Compound Breakout Tool: Asset classes = Futures, Options, and Stocks\n",
      "Wrote entries for Compound Breakout Tool\n",
      "Processing Voodoo Lines®: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Voodoo Lines®\n",
      "Processing Multi Squeeze Pro : Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for Multi Squeeze Pro \n",
      "Processing Volume Max Tool Package : Asset classes = Futures and Select Equities \n",
      "Wrote entries for Volume Max Tool Package \n",
      "Processing Earnings Hot Zone : Asset classes = Options, Futures, Stocks, and Crypto\n",
      "Wrote entries for Earnings Hot Zone \n",
      "Processing 10x Bars Indicator : Asset classes = Futures, ETF's, Stocks, and Options\n",
      "Wrote entries for 10x Bars Indicator \n",
      "Processing Drama Free Day Trades: Asset classes = Equities \n",
      "Wrote entries for Drama Free Day Trades\n",
      "Processing DynaRange Indicator : Asset classes = Futures and Equities \n",
      "Wrote entries for DynaRange Indicator \n",
      "Processing Simpler Options 101: Asset classes = Options\n",
      "Wrote entries for Simpler Options 101\n",
      "Processing John Carter’s Trend Rotation Tool: Asset classes = Futures and Equities \n",
      "Wrote entries for John Carter’s Trend Rotation Tool\n",
      "Processing Early In N Out Pro: Asset classes = Options\n",
      "Wrote entries for Early In N Out Pro\n",
      "Processing Multi 10x Indicator: Asset classes = Options\n",
      "Wrote entries for Multi 10x Indicator\n",
      "Processing Micro Voodoo Lines Indicator : Asset classes = Options, Futures, and Stocks & Crypto\n",
      "Wrote entries for Micro Voodoo Lines Indicator \n",
      "Processing Squeeze Rotation Tool : Asset classes = Futures and Equities \n",
      "Wrote entries for Squeeze Rotation Tool \n",
      "Processing Squeeze Pro Stats Tool : Asset classes = Futures and select Equities \n",
      "Wrote entries for Squeeze Pro Stats Tool \n",
      "Warning: No asset class information for Chart Patterns Mastery Trial \n",
      "Processing Compound Growth Mastery Trial : Asset classes = Options, Futures, and Equities\n",
      "Wrote entries for Compound Growth Mastery Trial \n",
      "Processing The Overnight Earnings Method: Asset classes = Equities and Futures \n",
      "Wrote entries for The Overnight Earnings Method\n",
      "Processing MEM Edge Report : Asset classes = Options, Futures, and Equities\n",
      "Wrote entries for MEM Edge Report \n",
      "JSONL file creation process completed.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def process_asset_classes_to_jsonl(input_csv, output_jsonl):\n",
    "    def format_asset_classes(classes):\n",
    "        if not classes or classes == \"N/A\":\n",
    "            return classes\n",
    "        class_list = [c.strip() for c in classes.split(',')]\n",
    "        if len(class_list) == 1:\n",
    "            return classes\n",
    "        elif len(class_list) == 2:\n",
    "            return ' and '.join(class_list)\n",
    "        else:\n",
    "            return ', '.join(class_list[:-1]) + ', and ' + class_list[-1]\n",
    "\n",
    "    question_templates = [\n",
    "        \"What asset classes is {product} traded in?\",\n",
    "        \"Which financial instruments can be traded using {product}?\",\n",
    "        \"Does {product} support trading in stocks, options, futures, or other asset classes?\",\n",
    "        \"What types of securities can be traded with {product}?\",\n",
    "        \"In terms of asset classes, what is the scope of {product}?\",\n",
    "        \"For which asset classes is {product} designed?\",\n",
    "        \"Can you tell me about the asset classes compatible with {product}?\",\n",
    "        \"What range of financial instruments does {product} cover?\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Opening CSV file: {input_csv}\")\n",
    "    with open(input_csv, 'r', encoding='utf-8') as csv_file, open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        rows = list(csv_reader)\n",
    "\n",
    "        print(\"Processing product names...\")\n",
    "        product_names = [name for name in rows[3][4:] if name]\n",
    "        print(f\"Found {len(product_names)} product names.\")\n",
    "\n",
    "        print(\"Processing asset classes...\")\n",
    "        asset_class_row = rows[6][4:]  # Start from column E (index 4)\n",
    "        \n",
    "        for i, (product, asset_classes) in enumerate(zip(product_names, asset_class_row)):\n",
    "            if asset_classes:\n",
    "                formatted_classes = format_asset_classes(asset_classes)\n",
    "                print(f\"Processing {product}: Asset classes = {formatted_classes}\")\n",
    "                \n",
    "                for template in question_templates:\n",
    "                    question = template.format(product=product)\n",
    "                    answer = f\"{product} is traded in {formatted_classes}\"\n",
    "                    json_line = json.dumps({\"question\": question, \"answer\": answer})\n",
    "                    jsonl_file.write(json_line + '\\n')\n",
    "                \n",
    "                print(f\"Wrote entries for {product}\")\n",
    "            else:\n",
    "                print(f\"Warning: No asset class information for {product}\")\n",
    "\n",
    "    print(\"JSONL file creation process completed.\")\n",
    "\n",
    "# Usage\n",
    "process_asset_classes_to_jsonl('/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv', 'asset_classes_output.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## product description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qa_pairs_product_description.jsonl'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with updated logic using E14 for the detailed description\n",
    "def generate_qa_pairs_updated_v3(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column E (index 4)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[13, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"What is {product_name}?\",\n",
    "            f\"Can you explain {product_name}?\",\n",
    "            f\"What are the benefits of {product_name}?\",\n",
    "            f\"How does {product_name} work?\",\n",
    "            f\"Can you describe the {product_name} strategy?\"\n",
    "        ]\n",
    "        \n",
    "        answer = f\"{product_name} - {detailed_description}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_updated_v3(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = 'qa_pairs_product_description.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qa_pairs_cleaned.jsonl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to clean up the JSONL file by removing entries with \"can be used on nan\"\n",
    "def cleanup_jsonl(input_file_path, output_file_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            if \"can be used on nan\" not in entry['answer']:\n",
    "                cleaned_data.append(entry)\n",
    "\n",
    "    # Write the cleaned data back to a new JSONL file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in cleaned_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = '/home/dave/Desktop/AI/pretraining/datasetformater/simpler/qa_pairs_product_description.jsonl'\n",
    "output_file_path = 'qa_pairs_cleaned.jsonl'\n",
    "\n",
    "# Run the cleanup function\n",
    "cleanup_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# Output the path of the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulleted list of what tool/strategy/service is designed to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qa_pairs_e4_e16.jsonl'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with variations using E4 and E16\n",
    "def generate_qa_pairs_e4_e16_variations(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column D (index 3)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[15, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"What is {product_name} designed to do?\",\n",
    "            f\"What are the key features of {product_name}?\",\n",
    "            f\"What functionalities does {product_name} offer?\",\n",
    "            f\"How does {product_name} work?\",\n",
    "            f\"Can you describe the primary purpose of {product_name}?\",\n",
    "            f\"What is the main goal of {product_name}?\",\n",
    "            f\"What benefits does {product_name} provide?\",\n",
    "            f\"How is {product_name} designed to help users?\",\n",
    "            f\"What problems does {product_name} aim to solve?\"\n",
    "        ]\n",
    "        \n",
    "        answer = f\"The {product_name} - {detailed_description}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_e4_e16_variations(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = 'qa_pairs_e4_e16.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleanup nans bulleted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cleaned_bulleted_qa_pairs_e4_e16.jsonl'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to clean up the JSONL file by removing entries with \"can be used on nan\"\n",
    "def cleanup_jsonl(input_file_path, output_file_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            if \"- nan\" not in entry['answer']:\n",
    "                cleaned_data.append(entry)\n",
    "\n",
    "    # Write the cleaned data back to a new JSONL file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in cleaned_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = '/home/dave/Desktop/AI/pretraining/datasetformater/simpler/qa_pairs_e4_e16.jsonl'\n",
    "output_file_path = 'cleaned_bulleted_qa_pairs_e4_e16.jsonl'\n",
    "\n",
    "# Run the cleanup function\n",
    "cleanup_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# Output the path of the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./raw/who_are_tradersqa_pairs_e4_e5.jsonl'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with variations using E4 and E16\n",
    "def generate_qa_pairs_e4_e5_variations(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column D (index 3)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[4, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"Who created {product_name}?\",\n",
    "            f\"who are the traders of {product_name}?\",\n",
    "          \n",
    "        ]\n",
    "        \n",
    "        answer = f\"{detailed_description} is the brains behind {product_name}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_e4_e5_variations(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = './raw/who_are_tradersqa_pairs_e4_e5.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is this strategy for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./raw/who_is_product_for_tradersqa_pairs_e4_e17.jsonl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with variations using E4 and E16\n",
    "def generate_qa_pairs_e4_e17_variations(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column D (index 3)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[16, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"Who is {product_name} for?\",\n",
    "            f\"what kind of trader is {product_name} best geared to?\",\n",
    "            f\"Who would benefit the most from using {product_name}?\",\n",
    "          \n",
    "        ]\n",
    "        \n",
    "        answer = f\"{detailed_description}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_e4_e17_variations(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = './raw/who_is_product_for_tradersqa_pairs_e4_e17.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./raw/cleaned_who_is_product_for_tradersqa_pairs_e4_e17.jsonl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to clean up the JSONL file by removing entries with \"can be used on nan\"\n",
    "def cleanup_jsonl(input_file_path, output_file_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            if \"nan\" not in entry['answer']:\n",
    "                cleaned_data.append(entry)\n",
    "\n",
    "    # Write the cleaned data back to a new JSONL file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in cleaned_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = './raw/who_is_product_for_tradersqa_pairs_e4_e17.jsonl'\n",
    "output_file_path = './raw/cleaned_who_is_product_for_tradersqa_pairs_e4_e17.jsonl'\n",
    "\n",
    "# Run the cleanup function\n",
    "cleanup_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# Output the path of the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued-Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/home/dave/Desktop/AI/pretraining/data/simpler/finetuning/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate the desired JSONL format\n",
    "def generate_jsonl_format(df):\n",
    "    jsonl_data = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column E (index 4)\n",
    "        product = f\"Simpler Trading Product: {df.iloc[3, col]}\"  # E4 (index 3 - 1)\n",
    "        body = (\n",
    "            f\"{df.iloc[3, col]}\\n\\n\"  # E4\n",
    "            f\"By: \\n{df.iloc[4, col]}\\n\\n\"  # E5\n",
    "            f\"Available on: \\n{df.iloc[5, col]}\\n\\n\"  # E6\n",
    "            f\"This product is designed for: \\n{df.iloc[6, col]}\\n\\n\"  # E7\n",
    "            f\"{df.iloc[7, col]}\\n\\n\"  # E8\n",
    "            f\"{df.iloc[11, col]}\\n\\n\"  # E12\n",
    "            f\"{df.iloc[12, col]}\\n\\n\"  # E13\n",
    "            f\"{df.iloc[13, col]}\\n\\n\"  # E14\n",
    "            f\"{df.iloc[14, col]}\\n\\n\"  # E15\n",
    "            f\"{df.iloc[15, col]}\\n\\n\"  # E16\n",
    "            f\"{df.iloc[16, col]}\\n\\n\"  # E17\n",
    "            f\"{df.iloc[17, col]}\\n\\n\"  # E18\n",
    "            f\"{df.iloc[18, col]}\\n\\n\"  # E19\n",
    "            f\"{df.iloc[19, col]}\\n\\n\"  # E20\n",
    "            f\"{df.iloc[20, col]}\\n\\n\"  # E21\n",
    "            f\"{df.iloc[21, col]}\\n\\n\"  # E22\n",
    "            f\"{df.iloc[22, col]}\\n\\n\"  # E23\n",
    "            f\"{df.iloc[23, col]}\\n\\n\"  # E24\n",
    "            f\"{df.iloc[24, col]}\\n\\n\"  # E25\n",
    "            f\"{df.iloc[25, col]}\\n\\n\"  # E26\n",
    "            f\"{df.iloc[26, col]}\\n\\n\"  # E27\n",
    "            f\"{df.iloc[27, col]}\\n\\n\"  # E28\n",
    "            f\"{df.iloc[28, col]}\\n\\n\"  # E29\n",
    "            f\"{df.iloc[29, col]}\\n\\n\"  # E30\n",
    "            f\"{df.iloc[30, col]}\\n\\n\"  # E31\n",
    "            f\"{df.iloc[31, col]}\\n\\n\"  # E32\n",
    "            f\"{df.iloc[32, col]}\\n\\n\"  # E33\n",
    "            f\"{df.iloc[33, col]}\\n\\n\"  # E34\n",
    "            f\"{df.iloc[34, col]}\\n\\n\"  # E35\n",
    "            f\"{df.iloc[35, col]}\\n\\n\"  # E36\n",
    "            f\"{df.iloc[36, col]}\\n\\n\"  # E37\n",
    "            f\"{df.iloc[37, col]}\\n\\n\"  # E38\n",
    "            f\"{df.iloc[38, col]}\\n\\n\"  # E39\n",
    "            f\"{df.iloc[3, col]} Testimonials: \\n\\n\"  # E4\n",
    "            f\"{df.iloc[39, col]}\\n\\n\"  # E40\n",
    "            f\"{df.iloc[40, col]}\\n\\n\"  # E41\n",
    "            f\"{df.iloc[41, col]}\\n\\n\"  # E42\n",
    "            f\"{df.iloc[42, col]}\\n\\n\"  # E43\n",
    "            f\"{df.iloc[43, col]}\\n\\n\"  # E44\n",
    "            f\"{df.iloc[44, col]}\\n\\n\"  # E45\n",
    "            f\"{df.iloc[45, col]}\\n\\n\"  # E46\n",
    "            f\"{df.iloc[46, col]}\\n\\n\"  # E47\n",
    "            f\"{df.iloc[47, col]}\\n\\n\"  # E48\n",
    "            f\"{df.iloc[48, col]}\\n\\n\"  # E49\n",
    "            f\"{df.iloc[49, col]}\\n\\n\"  # E50\n",
    "            f\"{df.iloc[40, col]}\\n\\n\"  # E51\n",
    "            f\"{df.iloc[3, col]} Frequently Asked Questions: \\n\\n\"  # E4\n",
    "            f\"{df.iloc[51, col]}\\n\\n\"  # E52\n",
    "            f\"{df.iloc[52, col]}\\n\\n\"  # E53\n",
    "            f\"{df.iloc[53, col]}\\n\\n\"  # E54\n",
    "            f\"{df.iloc[54, col]}\\n\\n\"  # E55\n",
    "            f\"{df.iloc[55, col]}\\n\\n\"  # E56\n",
    "            f\"{df.iloc[56, col]}\\n\\n\"  # E57\n",
    "            f\"{df.iloc[57, col]}\\n\\n\"  # E58\n",
    "            f\"{df.iloc[58, col]}\\n\\n\"  # E59\n",
    "        )\n",
    "\n",
    "        jsonl_data.append({\"product\": product, \"body\": body})\n",
    "    \n",
    "    return jsonl_data\n",
    "\n",
    "# Generate the JSONL data\n",
    "jsonl_data = generate_jsonl_format(df)\n",
    "\n",
    "# Save the data to a JSONL file\n",
    "output_file_path = 'pretrain.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for item in jsonl_data:\n",
    "        json.dump(item, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "print(output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add a reference to simpler trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2 column 1 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/dave/Desktop/AI/pretraining/datasetformater/simpler/finetune/all.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-qa-final.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodify_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mmodify_questions\u001b[0;34m(input_file, output_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile, \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m infile:\n\u001b[0;32m----> 6\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccording to Simpler Trading, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data, outfile)\n",
      "File \u001b[0;32m~/miniconda3/envs/pretrainold/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/pretrainold/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/pretrainold/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def modify_questions(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            data['question'] = f\"According to Simpler Trading, {data['question']}\"\n",
    "            json.dump(data, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Usage\n",
    "input_file = '/home/dave/Desktop/AI/pretraining/datasetformater/simpler/finetune/all.jsonl'\n",
    "output_file = 'all-qa-final.jsonl'\n",
    "modify_questions(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrainold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
