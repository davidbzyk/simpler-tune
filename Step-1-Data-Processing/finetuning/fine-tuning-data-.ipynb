{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions go down the list row by row in the `simplerrawdata.csv` and picks out items to forms question and answer pairs.  Python index starts at 0, and we start with sandbox in E4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What platform can {product} be used with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_jsonl(input_csv, output_jsonl):\n",
    "    def format_platforms(platforms):\n",
    "        platforms_list = platforms.split('|')\n",
    "        if len(platforms_list) == 1:\n",
    "            return platforms\n",
    "        elif len(platforms_list) == 2:\n",
    "            return ' and '.join(platforms_list)\n",
    "        else:\n",
    "            return ', '.join(platforms_list[:-1]) + ', and ' + platforms_list[-1]\n",
    "\n",
    "    print(f\"Opening CSV file: {input_csv}\")\n",
    "    with open(input_csv, 'r', encoding='utf-8') as csv_file, open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        print(\"Skipping first 3 rows...\")\n",
    "        next(csv_reader)  # Skip the first 3 rows\n",
    "        next(csv_reader)\n",
    "        next(csv_reader)\n",
    "        product_names = next(csv_reader)[4:]  # Get product names from row 4, starting from column E\n",
    "        print(f\"Found {len(product_names)} product names: {product_names[:5]}...\")\n",
    "\n",
    "        print(\"Searching for the platforms row...\")\n",
    "        for row_num, row in enumerate(csv_reader, start=5):\n",
    "            print(f\"Checking row {row_num}: {row[:5]}...\")\n",
    "            if row and len(row) > 2 and \"What platforms can\" in row[2]:\n",
    "                print(f\"Found platforms row: {row[:5]}...\")\n",
    "                for i, product in enumerate(product_names):\n",
    "                    if i + 4 < len(row) and row[i+4]:  # Check if there's a value for this product\n",
    "                        question = f\"What platforms can {product} be used on?\"\n",
    "                        answer = f\"{product} can be used on {format_platforms(row[i+4])}\"\n",
    "                        json_line = json.dumps({\"question\": question, \"answer\": answer})\n",
    "                        jsonl_file.write(json_line + '\\n')\n",
    "                        print(f\"Wrote entry for {product}\")\n",
    "                print(\"Finished processing platforms row\")\n",
    "                break  # We've found the row we need, no need to continue\n",
    "        else:\n",
    "            print(\"WARNING: Did not find a row containing platform information!\")\n",
    "\n",
    "    print(\"JSONL file creation process completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_csv_to_jsonl('../raw-data/simplerrawdata.csv', './processed/products.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its good to rephrase the same question multiple times so the model learns different variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_csv_to_jsonl(input_csv, output_jsonl):\n",
    "    def format_platforms(platforms):\n",
    "        if not platforms or platforms == \"N/A\":\n",
    "            return platforms\n",
    "        platforms_list = [p.strip() for p in platforms.split('|')]\n",
    "        if len(platforms_list) == 1:\n",
    "            return platforms\n",
    "        elif len(platforms_list) == 2:\n",
    "            return ' and '.join(platforms_list)\n",
    "        else:\n",
    "            return ', '.join(platforms_list[:-1]) + ', and ' + platforms_list[-1]\n",
    "\n",
    "    question_templates = [\n",
    "        \"What platforms can the {product} be used on?\",\n",
    "        \"On which platforms can {product} be used?\",\n",
    "        \"Where is {product} available for use?\",\n",
    "        \"If I want to use {product}, which platforms should I have access to?\",\n",
    "        \"What platform features are required to run {product}?\",\n",
    "        \"How does the platform compatibility of {product} compare to other products?\",\n",
    "        \"Are there any major platforms that don't support {product}?\",\n",
    "        \"Which platforms are compatible with {product}?\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Opening CSV file: {input_csv}\")\n",
    "    with open(input_csv, 'r', encoding='utf-8') as csv_file, open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        rows = list(csv_reader)\n",
    "\n",
    "        print(\"Processing product names...\")\n",
    "        product_names = [name for name in rows[3][4:] if name]\n",
    "        print(f\"Found {len(product_names)} product names.\")\n",
    "\n",
    "        print(\"Searching for the platforms row...\")\n",
    "        for row in rows:\n",
    "            if row and len(row) > 2 and \"What platforms can the\" in row[2]:\n",
    "                print(\"Found platforms row.\")\n",
    "                for i, product in enumerate(product_names):\n",
    "                    if i + 4 < len(row) and row[i+4]:\n",
    "                        platforms = format_platforms(row[i+4])\n",
    "                        for template in question_templates:\n",
    "                            question = template.format(product=product)\n",
    "                            answer = f\"{product} can be used on {platforms}\"\n",
    "                            json_line = json.dumps({\"question\": question, \"answer\": answer})\n",
    "                            jsonl_file.write(json_line + '\\n')\n",
    "                        print(f\"Wrote entries for {product}\")\n",
    "                print(\"Finished processing platforms row\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"WARNING: Did not find a row containing platform information!\")\n",
    "\n",
    "    print(\"JSONL file creation process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "process_csv_to_jsonl('../raw-data/simplerrawdata.csv', './processed/multiqproducts.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening CSV file: ../raw-data/simplerrawdata.csv\n",
      "Processing product names...\n",
      "Found 68 product names.\n",
      "Processing asset classes...\n",
      "Processing John Carter's Sandbox Strategy: Asset classes = Options\n",
      "Wrote entries for John Carter's Sandbox Strategy\n",
      "Processing Top Tier Pro System: Asset classes = All asset classes but most trading done in Options\n",
      "Wrote entries for Top Tier Pro System\n",
      "Processing The PMZ System: Asset classes = Options, Futures, Stocks, and 0DTE\n",
      "Wrote entries for The PMZ System\n",
      "Processing The New “Big 3” Squeeze Master Class: Asset classes = Options, Stocks, and Futures\n",
      "Wrote entries for The New “Big 3” Squeeze Master Class\n",
      "Processing Day Trading with Tr3ndy Zones: Asset classes = 0DTE, Options, Futures, and Stocks\n",
      "Wrote entries for Day Trading with Tr3ndy Zones\n",
      "Processing Micro Voodoo Line Strategy: Asset classes = Options and Futures\n",
      "Wrote entries for Micro Voodoo Line Strategy\n",
      "Processing DPMR Masterclass: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for DPMR Masterclass\n",
      "Processing Chart Patterns Secrets: Asset classes = Stocks, Options, and Futures\n",
      "Wrote entries for Chart Patterns Secrets\n",
      "Processing The Stacked Profit Strategy: Asset classes = Options\n",
      "Wrote entries for The Stacked Profit Strategy\n",
      "Processing The Multi Squeeze Pro System: Asset classes = Options\n",
      "Wrote entries for The Multi Squeeze Pro System\n",
      "Processing Raghee's Day Trading Options Strategy Course: Asset classes = Options\n",
      "Wrote entries for Raghee's Day Trading Options Strategy Course\n",
      "Processing Tr3ndy Jon’s Supply\n",
      "& Demand System: Asset classes = Options, Futures, Stocks, and 0DTE\n",
      "Wrote entries for Tr3ndy Jon’s Supply\n",
      "& Demand System\n",
      "Processing The Ready. Aim. Fire!® Pro System\n",
      ": Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for The Ready. Aim. Fire!® Pro System\n",
      "\n",
      "Processing True Momentum System: Asset classes = Options, Stocks, Futures, and ETFs\n",
      "Wrote entries for True Momentum System\n",
      "Processing Bulletproof Butterflies 2.0: Asset classes = Options and Stocks\n",
      "Wrote entries for Bulletproof Butterflies 2.0\n",
      "Processing Decoding Volume: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Decoding Volume\n",
      "Processing 5 Star Options Income Plan: Asset classes = Options\n",
      "Wrote entries for 5 Star Options Income Plan\n",
      "Processing The Compound Butterfly Blueprint: Asset classes = Options\n",
      "Wrote entries for The Compound Butterfly Blueprint\n",
      "Processing Pocketing Premium Masterclass: Asset classes = Options\n",
      "Wrote entries for Pocketing Premium Masterclass\n",
      "Processing The Moxie Indicator: Asset classes = Stock, Options, Futures, and 0DTE\n",
      "Wrote entries for The Moxie Indicator\n",
      "Processing Strike Zone Strategy: Asset classes = Futures\n",
      "Wrote entries for Strike Zone Strategy\n",
      "Processing Options Freedom Formula: Asset classes = Options\n",
      "Wrote entries for Options Freedom Formula\n",
      "Processing Small Account Futures: Asset classes = Futures\n",
      "Wrote entries for Small Account Futures\n",
      "Processing Small Account Blueprint: Asset classes = Options\n",
      "Wrote entries for Small Account Blueprint\n",
      "Processing Quarterly Profits Formula: Asset classes = Options\n",
      "Wrote entries for Quarterly Profits Formula\n",
      "Processing Ultimate Spread Strategy: Asset classes = Stocks \n",
      "Wrote entries for Ultimate Spread Strategy\n",
      "Processing Next Level Futures: Asset classes = Options, Stocks, Indexes, ETFs, and and Futures\n",
      "Wrote entries for Next Level Futures\n",
      "Processing Volume Breakout System: Asset classes = Futures\n",
      "Wrote entries for Volume Breakout System\n",
      "Processing True Low Formula: Asset classes = Futures\n",
      "Wrote entries for True Low Formula\n",
      "Processing TG's $100K Day Setup: Asset classes = Options\n",
      "Wrote entries for TG's $100K Day Setup\n",
      "Processing Phoenix Finder Turbo: Asset classes = Stocks\n",
      "Wrote entries for Phoenix Finder Turbo\n",
      "Processing The Obnoxious Profit Strategy: Asset classes = Futures, Options, and Stocks\n",
      "Wrote entries for The Obnoxious Profit Strategy\n",
      "Warning: No asset class information for Simpler Calendars\n",
      "Processing New Multi 10X on Steroids: Asset classes = Futures/Options/ETFs\n",
      "Wrote entries for New Multi 10X on Steroids\n",
      "Processing John Carter's Squeeze Pro System: Asset classes = Options\n",
      "Wrote entries for John Carter's Squeeze Pro System\n",
      "Processing Ready.Aim.Fire!® (RAF) Pro Indicator: Asset classes = Options\n",
      "Wrote entries for Ready.Aim.Fire!® (RAF) Pro Indicator\n",
      "Processing John Carter's Squeeze Pro Indicator: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for John Carter's Squeeze Pro Indicator\n",
      "Processing Raghee Horner’s DPMR Indicator: Asset classes = Options\n",
      "Wrote entries for Raghee Horner’s DPMR Indicator\n",
      "Processing Tr3ndy Script Bundle: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Tr3ndy Script Bundle\n",
      "Processing Raghee Horner’s HPMR Indicator: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for Raghee Horner’s HPMR Indicator\n",
      "Processing TG's Moxie Indicator + Scans: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for TG's Moxie Indicator + Scans\n",
      "Processing Quant Pivots: Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for Quant Pivots\n",
      "Processing Darvas Box 3.0: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Darvas Box 3.0\n",
      "Processing True Low Indicator: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for True Low Indicator\n",
      "Processing Trend Spark Indicator: Asset classes = Futures, Options, and Stocks\n",
      "Wrote entries for Trend Spark Indicator\n",
      "Processing Volume Max Tool Kit (formerly VWAP): Asset classes = Stocks, Options, Futures, and ETFs\n",
      "Wrote entries for Volume Max Tool Kit (formerly VWAP)\n",
      "Processing Phoenix Finder: Asset classes = Futures\n",
      "Wrote entries for Phoenix Finder\n",
      "Processing Turbo VZO: Asset classes = Futures, ETF's, Stocks, and Options\n",
      "Wrote entries for Turbo VZO\n",
      "Processing Divergent Bar: Asset classes = Stocks, Options, and ETFs\n",
      "Wrote entries for Divergent Bar\n",
      "Processing Compound Breakout Tool: Asset classes = Futures, Options, and Stocks\n",
      "Wrote entries for Compound Breakout Tool\n",
      "Processing Voodoo Lines®: Asset classes = Options, Futures, and Stocks\n",
      "Wrote entries for Voodoo Lines®\n",
      "Processing Multi Squeeze Pro : Asset classes = Options, Futures, Stocks, and ETFs\n",
      "Wrote entries for Multi Squeeze Pro \n",
      "Processing Volume Max Tool Package : Asset classes = Futures and Select Equities \n",
      "Wrote entries for Volume Max Tool Package \n",
      "Processing Earnings Hot Zone : Asset classes = Options, Futures, Stocks, and Crypto\n",
      "Wrote entries for Earnings Hot Zone \n",
      "Processing 10x Bars Indicator : Asset classes = Futures, ETF's, Stocks, and Options\n",
      "Wrote entries for 10x Bars Indicator \n",
      "Processing Drama Free Day Trades: Asset classes = Equities \n",
      "Wrote entries for Drama Free Day Trades\n",
      "Processing DynaRange Indicator : Asset classes = Futures and Equities \n",
      "Wrote entries for DynaRange Indicator \n",
      "Processing Simpler Options 101: Asset classes = Options\n",
      "Wrote entries for Simpler Options 101\n",
      "Processing John Carter’s Trend Rotation Tool: Asset classes = Futures and Equities \n",
      "Wrote entries for John Carter’s Trend Rotation Tool\n",
      "Processing Early In N Out Pro: Asset classes = Options\n",
      "Wrote entries for Early In N Out Pro\n",
      "Processing Multi 10x Indicator: Asset classes = Options\n",
      "Wrote entries for Multi 10x Indicator\n",
      "Processing Micro Voodoo Lines Indicator : Asset classes = Options, Futures, and Stocks & Crypto\n",
      "Wrote entries for Micro Voodoo Lines Indicator \n",
      "Processing Squeeze Rotation Tool : Asset classes = Futures and Equities \n",
      "Wrote entries for Squeeze Rotation Tool \n",
      "Processing Squeeze Pro Stats Tool : Asset classes = Futures and select Equities \n",
      "Wrote entries for Squeeze Pro Stats Tool \n",
      "Warning: No asset class information for Chart Patterns Mastery Trial \n",
      "Processing Compound Growth Mastery Trial : Asset classes = Options, Futures, and Equities\n",
      "Wrote entries for Compound Growth Mastery Trial \n",
      "Processing The Overnight Earnings Method: Asset classes = Equities and Futures \n",
      "Wrote entries for The Overnight Earnings Method\n",
      "Processing MEM Edge Report : Asset classes = Options, Futures, and Equities\n",
      "Wrote entries for MEM Edge Report \n",
      "JSONL file creation process completed.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def process_asset_classes_to_jsonl(input_csv, output_jsonl):\n",
    "    def format_asset_classes(classes):\n",
    "        if not classes or classes == \"N/A\":\n",
    "            return classes\n",
    "        class_list = [c.strip() for c in classes.split(',')]\n",
    "        if len(class_list) == 1:\n",
    "            return classes\n",
    "        elif len(class_list) == 2:\n",
    "            return ' and '.join(class_list)\n",
    "        else:\n",
    "            return ', '.join(class_list[:-1]) + ', and ' + class_list[-1]\n",
    "\n",
    "    question_templates = [\n",
    "        \"What asset classes is {product} traded in?\",\n",
    "        \"Which financial instruments can be traded using {product}?\",\n",
    "        \"Does {product} support trading in stocks, options, futures, or other asset classes?\",\n",
    "        \"What types of securities can be traded with {product}?\",\n",
    "        \"In terms of asset classes, what is the scope of {product}?\",\n",
    "        \"For which asset classes is {product} designed?\",\n",
    "        \"Can you tell me about the asset classes compatible with {product}?\",\n",
    "        \"What range of financial instruments does {product} cover?\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Opening CSV file: {input_csv}\")\n",
    "    with open(input_csv, 'r', encoding='utf-8') as csv_file, open(output_jsonl, 'w', encoding='utf-8') as jsonl_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        rows = list(csv_reader)\n",
    "\n",
    "        print(\"Processing product names...\")\n",
    "        product_names = [name for name in rows[3][4:] if name]\n",
    "        print(f\"Found {len(product_names)} product names.\")\n",
    "\n",
    "        print(\"Processing asset classes...\")\n",
    "        asset_class_row = rows[6][4:]  # Start from column E (index 4)\n",
    "        \n",
    "        for i, (product, asset_classes) in enumerate(zip(product_names, asset_class_row)):\n",
    "            if asset_classes:\n",
    "                formatted_classes = format_asset_classes(asset_classes)\n",
    "                print(f\"Processing {product}: Asset classes = {formatted_classes}\")\n",
    "                \n",
    "                for template in question_templates:\n",
    "                    question = template.format(product=product)\n",
    "                    answer = f\"{product} is traded in {formatted_classes}\"\n",
    "                    json_line = json.dumps({\"question\": question, \"answer\": answer})\n",
    "                    jsonl_file.write(json_line + '\\n')\n",
    "                \n",
    "                print(f\"Wrote entries for {product}\")\n",
    "            else:\n",
    "                print(f\"Warning: No asset class information for {product}\")\n",
    "\n",
    "    print(\"JSONL file creation process completed.\")\n",
    "\n",
    "# Usage\n",
    "process_asset_classes_to_jsonl('../raw-data/simplerrawdata.csv', './processed/asset_classes_output.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## product description \n",
    "I switched to pandas to see if it was more efficient.. not much.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/qa_pairs_product_description.jsonl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../raw-data/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with updated logic using E14 for the detailed description\n",
    "def generate_qa_pairs_updated_v3(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column E (index 4)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[13, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"What is {product_name}?\",\n",
    "            f\"Can you explain {product_name}?\",\n",
    "            f\"What are the benefits of {product_name}?\",\n",
    "            f\"How does {product_name} work?\",\n",
    "            f\"Can you describe the {product_name} strategy?\"\n",
    "        ]\n",
    "        \n",
    "        answer = f\"{product_name} - {detailed_description}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_updated_v3(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = './processed/qa_pairs_product_description.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There were a bunch of blank rows for some of product descriptions so we need to clean up the nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cleaned/qa_pairs_cleaned.jsonl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to clean up the JSONL file by removing entries with \"can be used on nan\"\n",
    "def cleanup_jsonl(input_file_path, output_file_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            if \"- nan\" not in entry['answer']:\n",
    "                cleaned_data.append(entry)\n",
    "\n",
    "    # Write the cleaned data back to a new JSONL file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in cleaned_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = './processed/qa_pairs_product_description.jsonl'\n",
    "output_file_path = './cleaned/qa_pairs_cleaned.jsonl'\n",
    "\n",
    "# Run the cleanup function\n",
    "cleanup_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# Output the path of the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulleted list of what tool/strategy/service is designed to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/qa_pairs_e4_e16.jsonl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../raw-data/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with variations using E4 and E16\n",
    "def generate_qa_pairs_e4_e16_variations(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column D (index 3)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[15, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"What is {product_name} designed to do?\",\n",
    "            f\"What are the key features of {product_name}?\",\n",
    "            f\"What functionalities does {product_name} offer?\",\n",
    "            f\"How does {product_name} work?\",\n",
    "            f\"Can you describe the primary purpose of {product_name}?\",\n",
    "            f\"What is the main goal of {product_name}?\",\n",
    "            f\"What benefits does {product_name} provide?\",\n",
    "            f\"How is {product_name} designed to help users?\",\n",
    "            f\"What problems does {product_name} aim to solve?\"\n",
    "        ]\n",
    "        \n",
    "        answer = f\"The {product_name} - {detailed_description}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_e4_e16_variations(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = './processed/qa_pairs_e4_e16.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleanup nans bulleted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cleaned/cleaned_bulleted_qa_pairs_e4_e16.jsonl'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to clean up the JSONL file by removing entries with \"can be used on nan\"\n",
    "def cleanup_jsonl(input_file_path, output_file_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            if \"- nan\" not in entry['answer']:\n",
    "                cleaned_data.append(entry)\n",
    "\n",
    "    # Write the cleaned data back to a new JSONL file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in cleaned_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = './processed/qa_pairs_e4_e16.jsonl'\n",
    "output_file_path = './cleaned/cleaned_bulleted_qa_pairs_e4_e16.jsonl'\n",
    "\n",
    "# Run the cleanup function\n",
    "cleanup_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# Output the path of the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## who are traders on product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/who_are_tradersqa_pairs_e4_e5.jsonl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../raw-data/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with variations using E4 and E16\n",
    "def generate_qa_pairs_e4_e5_variations(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column D (index 3)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[4, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"Who created {product_name}?\",\n",
    "            f\"who are the traders of {product_name}?\",\n",
    "          \n",
    "        ]\n",
    "        \n",
    "        answer = f\"{detailed_description} is the brains behind {product_name}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_e4_e5_variations(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = './processed/who_are_tradersqa_pairs_e4_e5.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is this strategy for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/who_is_product_for_tradersqa_pairs_e4_e17.jsonl'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../raw-data/simplerrawdata.csv'\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Function to generate question-answer pairs with variations using E4 and E16\n",
    "def generate_qa_pairs_e4_e17_variations(df):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for col in range(4, df.shape[1]):  # Starting from column D (index 3)\n",
    "        product_name = df.iloc[3, col]\n",
    "        detailed_description = df.iloc[16, col]\n",
    "\n",
    "        questions = [\n",
    "            f\"Who is {product_name} for?\",\n",
    "            f\"what kind of trader is {product_name} best geared to?\",\n",
    "            f\"Who would benefit the most from using {product_name}?\",\n",
    "          \n",
    "        ]\n",
    "        \n",
    "        answer = f\"{detailed_description}\"\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate the QA pairs\n",
    "qa_pairs = generate_qa_pairs_e4_e17_variations(df)\n",
    "\n",
    "# Save the QA pairs to a JSONL file\n",
    "output_file_path = './processed/who_is_product_for_tradersqa_pairs_e4_e17.jsonl'\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for pair in qa_pairs:\n",
    "        json.dump(pair, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# Output the path of the generated file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "had some nans.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cleaned/cleaned_who_is_product_for_tradersqa_pairs_e4_e17.jsonl'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to clean up the JSONL file by removing entries with \"can be used on nan\"\n",
    "def cleanup_jsonl(input_file_path, output_file_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            if \"nan\" not in entry['answer']:\n",
    "                cleaned_data.append(entry)\n",
    "\n",
    "    # Write the cleaned data back to a new JSONL file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in cleaned_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = './processed/who_is_product_for_tradersqa_pairs_e4_e17.jsonl'\n",
    "output_file_path = './cleaned/cleaned_who_is_product_for_tradersqa_pairs_e4_e17.jsonl'\n",
    "\n",
    "# Run the cleanup function\n",
    "cleanup_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# Output the path of the cleaned file\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate files in directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated and cleaned file saved as concatenated-question-answer-pairs.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def concatenate_and_clean_jsonl_files(input_dirs, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for dir_path in input_dirs:\n",
    "            for filename in os.listdir(dir_path):\n",
    "                if filename.endswith('.jsonl'):\n",
    "                    file_path = os.path.join(dir_path, filename)\n",
    "                    with open(file_path, 'r') as infile:\n",
    "                        for line in infile:\n",
    "                            # Remove '/h2' and '/h3' from the line\n",
    "                            cleaned_line = line.replace('/h2', '').replace('/h3', '')\n",
    "                            outfile.write(cleaned_line)\n",
    "    print(f\"Concatenated and cleaned file saved as {output_file}\")\n",
    "\n",
    "# Define the input directories\n",
    "input_dirs = [\n",
    "    './processed',\n",
    "    './cleaned'\n",
    "]\n",
    "\n",
    "# Define the output file path\n",
    "output_file = 'concatenated-question-answer-pairs.jsonl'\n",
    "\n",
    "# Call the function to concatenate and clean the JSONL files\n",
    "concatenate_and_clean_jsonl_files(input_dirs, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add context to beginning of QA paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def modify_questions(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            data['question'] = f\"According to Simpler Trading, {data['question']}\"\n",
    "            json.dump(data, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Usage\n",
    "input_file = 'concatenated-question-answer-pairs.jsonl'\n",
    "output_file = 'all-qa-final.jsonl'\n",
    "modify_questions(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrainold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
