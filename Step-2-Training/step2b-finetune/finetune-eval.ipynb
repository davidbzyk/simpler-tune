{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TrainerCallback, TrainerState, TrainerControl, EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN='<INSERT HUGGING FACE API KEY>'\n",
    "WANDB_API_KEY='<INSERT WANDB API KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"simpler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to pull your own repo/model you saved in previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'davidbzyk/simpler-gemma-2-2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATH ='../../Step-1-Data-Processing/finetuning/all-qa-final.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(    \n",
    "    model_name = MODEL_NAME,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_texts(question, answer):\n",
    "    return {\n",
    "        \"text\": f\"###{question}@@@{answer}{EOS_TOKEN}\",\n",
    "    }\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_number, line in enumerate(f, start=1):\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    try:\n",
    "                        entry = json.loads(line)\n",
    "                        questions.append(entry['question'])\n",
    "                        answers.append(entry['answer'])\n",
    "                        print(f\"Successfully parsed line {line_number}\")\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON on line {line_number}: {e}\")\n",
    "                        print(f\"Problematic line: {line}\")\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Missing key in JSON entry on line {line_number}: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Unexpected error while reading the file: {e}\")\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from JSONL file\n",
    "try:\n",
    "    questions, answers = load_data_from_jsonl(TRAINING_DATA_PATH)\n",
    "    print(f\"Total parsed entries: {len(questions)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {e}\")\n",
    "    questions, answers = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and evaluation sets\n",
    "train_size = int(0.9 * len(questions))\n",
    "train_questions = questions[:train_size]\n",
    "train_answers = answers[:train_size]\n",
    "eval_questions = questions[train_size:]\n",
    "eval_answers = answers[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the fine-tuning training dataset\n",
    "if train_questions and train_answers:\n",
    "    combined_texts_train = [combine_texts(question, answer) for question, answer in zip(train_questions, train_answers)]\n",
    "    combined_texts_eval = [combine_texts(question, answer) for question, answer in zip(eval_questions, eval_answers)]\n",
    "\n",
    "    # Create the fine-tuning datasets\n",
    "    train_dataset = Dataset.from_dict({\"text\": [ct[\"text\"] for ct in combined_texts_train]})\n",
    "    eval_dataset = Dataset.from_dict({\"text\": [ct[\"text\"] for ct in combined_texts_eval]})\n",
    "\n",
    "    # Display example training record\n",
    "    if len(train_dataset) > 0:\n",
    "        print(\"Example training record:\\n\")\n",
    "        print(train_dataset[0]['text'])\n",
    "    else:\n",
    "        print(\"The fine-tuning training dataset is empty.\")\n",
    "    \n",
    "    if len(eval_dataset) > 0:\n",
    "        print(\"Example evaluation record:\\n\")\n",
    "        print(eval_dataset[0]['text'])\n",
    "    else:\n",
    "        print(\"The fine-tuning evaluation dataset is empty.\")\n",
    "else:\n",
    "    print(\"Failed to create the fine-tuning datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_qna = 0\n",
    "max_q = 0\n",
    "max_a = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    q_tokens = tokenizer.encode_plus(question, add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "    a_tokens = tokenizer.encode_plus(answer, add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "    qna_tokens = tokenizer.encode_plus(combine_texts(question, answer)[\"text\"], add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "\n",
    "    max_q = max(max_q, len(q_tokens))\n",
    "    max_a = max(max_a, len(a_tokens))\n",
    "    max_qna = max(max_qna, len(qna_tokens))\n",
    "\n",
    "buffer = 10\n",
    "max_seq_length = max_qna + buffer\n",
    "\n",
    "table_title = \"Training Data Token Counts\"\n",
    "print(f\"\\n{table_title:-^70}\")\n",
    "print(f\"{'Measure':<14}{'Question':<14}{'Answer':<14}{'Combined':<14}\")\n",
    "\n",
    "print(f\"{'Maximums':<14}{max_q:<14}{max_a:<14}{max_qna:<14}\")\n",
    "print(f\"{'Max Seq Len':<14}{'':<14}{'':<14}{max_seq_length:<14}\\n\")\n",
    "\n",
    "print(f\"Set max_seq_length in FastLanguageModel to {max_seq_length} to handle the maximum number of tokens required by the input training data (Combined Maximum + Buffer).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = None \n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(    \n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # add a Hugging Face access token if using a private or gated model\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear memory if needed.. if not needed.. no worries.. skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_cuda_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"CUDA memory cleared.\")\n",
    "\n",
    "# Call the function to clear memory\n",
    "clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration to wandb\n",
    "config = {\n",
    "    \"learning_rate\": 2e-5,    \n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"num_train_epochs\": 20,  # Increased to allow early stopping to take effect\n",
    "    \"warmup_steps\": 100,\n",
    "    \"max_seq_length\": max_seq_length,  # To be calculated later\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    num_train_epochs=config['num_train_epochs'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=5,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,  # Limit to 3 checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # Use evaluation loss to determine the best model\n",
    "    greater_is_better=False,  # Lower loss is better\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup callbacks for WANDB logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.log_history:\n",
    "            wandb.log(state.log_history[-1])\n",
    "\n",
    "    def on_train_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        wandb.init(project=\"your_project_name\", config=args)\n",
    "\n",
    "    def on_train_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        wandb.finish()\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.log_history:\n",
    "            wandb.log(state.log_history[-1])\n",
    "\n",
    "    def on_save(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        wandb.log({\"global_step\": state.global_step, \"saving_checkpoint\": True})\n",
    "\n",
    "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.log_history:\n",
    "            wandb.log(state.log_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with the enhanced WandbCallback\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Add evaluation dataset for monitoring\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    "    callbacks=[WandbCallback(), EarlyStoppingCallback(early_stopping_patience=4)],  # Add early stopping callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(MODEL_NAME,token=HUGGING_FACE_HUB_TOKEN)\n",
    "tokenizer.push_to_hub(MODEL_NAME,token=HUGGING_FACE_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "# Enable native faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Define the question\n",
    "question = \"Who is Raghee Horner?\"\n",
    "\n",
    "# Format the input\n",
    "formatted_input = question\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    [formatted_input],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the output using the model\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the below with how you want to save the model to huggingface/locally  p.s. locally you will fill the hard drive real quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"davidbzyk\"\n",
    "model_name = \"simpler-gemma-2-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "#if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "#if True: model.push_to_hub_gguf(\"{username}/{model_name}-{}\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "#if True: model.save_pretrained_gguf(f\"{username}/{model_name}-{quantization_method}\", tokenizer, quantization_method = \"f16\")\n",
    "if True: model.push_to_hub_gguf(f\"{username}/{model_name}-f16\", tokenizer, quantization_method = \"merged_16bit\", token =HUGGING_FACE_HUB_TOKEN)\n",
    "if True: model.push_to_hub_gguf(f\"{username}/{model_name}-f16\", tokenizer, quantization_method = \"f16\", token =HUGGING_FACE_HUB_TOKEN)\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "#if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if True: model.push_to_hub_gguf(f\"{username}/{model_name}-q4_k_m\", tokenizer, quantization_method = \"q4_k_m\", token =HUGGING_FACE_HUB_TOKEN)\n",
    "if True: model.push_to_hub_gguf(f\"{username}/{model_name}-q5_k_m\", tokenizer, quantization_method = \"q5_k_m\", token =HUGGING_FACE_HUB_TOKEN)\n",
    "if True: model.push_to_hub_gguf(f\"{username}/{model_name}-q8_0\", tokenizer, quantization_method = \"q8_0\", token =HUGGING_FACE_HUB_TOKEN)\n",
    "if True: model.push_to_hub_gguf(f\"{username}/{model_name}-q5_0\", tokenizer, quantization_method = \"q5_0\", token =HUGGING_FACE_HUB_TOKEN)\n",
    "if True: model.push_to_hub_gguf(f\"{username}/{model_name}-q5_1\", tokenizer, quantization_method = \"q5_1\", token =HUGGING_FACE_HUB_TOKEN)\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
