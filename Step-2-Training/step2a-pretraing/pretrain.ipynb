{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "import gc\n",
    "import wandb  #Remove if you arent interested in the analytics\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN=os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "WANDB_API_KEY=os.getenv(\"WANDB_API_KEY\")\n",
    "WANDB_NOTEBOK_NAME=os.getenv(\"WANDB_NOTEBOK_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find Simpler-Trading.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelraycapitalmanagement\u001b[0m (\u001b[33mdavidbzyk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dave/Desktop/simpler/simpler-prod-qa/Step-2-Training/step2a-pretraing/wandb/run-20240804_115005-gkw9nfow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/davidbzyk/Simpler/runs/gkw9nfow' target=\"_blank\">vital-star-12</a></strong> to <a href='https://wandb.ai/davidbzyk/Simpler' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/davidbzyk/Simpler' target=\"_blank\">https://wandb.ai/davidbzyk/Simpler</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/davidbzyk/Simpler/runs/gkw9nfow' target=\"_blank\">https://wandb.ai/davidbzyk/Simpler/runs/gkw9nfow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/davidbzyk/Simpler/runs/gkw9nfow?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7c74d4403e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Simpler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##USE THIS FOR FIRST TIME TRAINING OR PICK ANOTHER SMALL MODEL FROM https://huggingface.co/unsloth\n",
    "#MODEL_NAME=\"unsloth/gemma-2-2b-it-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='unsloth/gemma-2-2b-it-bnb-4bit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../Step-1-Data-Processing/pretraining/split-pretrain.jsonl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.668 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 550 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Lora adapter via peft ( this allows for updating only 1 to 10% of parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure to always add EOS_TOKEN or model will ramble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files=file_path, split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = examples[\"text\"]\n",
    "    outputs = []\n",
    "    for text in texts:\n",
    "        formatted_text = text + EOS_TOKEN\n",
    "        outputs.append(formatted_text)\n",
    "    return {\"text\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "feel free to play around with hyper parameters on both the lora adapters as well as the trainer arguments such as learning rate, epochs or steps, batch size or accumulation steps (memory intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
    "        #max_steps = 120,\n",
    "        #warmup_steps = 10,\n",
    "        warmup_ratio = 0.2,\n",
    "        num_train_epochs = 15,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        gradient_checkpointing=True,\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",       \n",
    "        report_to=\"wandb\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 184 | Num Epochs = 15\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 16 | Total steps = 165\n",
      " \"-____-\"     Number of trainable parameters = 1,345,781,760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n",
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00aacf660114403a936d1391581be38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1468, 'grad_norm': 7.354604244232178, 'learning_rate': 1.5151515151515152e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3101, 'grad_norm': 7.639742374420166, 'learning_rate': 3.0303030303030305e-06, 'epoch': 0.17}\n",
      "{'loss': 3.1517, 'grad_norm': 6.832216739654541, 'learning_rate': 4.5454545454545455e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2621, 'grad_norm': 7.00938606262207, 'learning_rate': 6.060606060606061e-06, 'epoch': 0.35}\n",
      "{'loss': 3.0344, 'grad_norm': 5.7711944580078125, 'learning_rate': 7.5757575757575764e-06, 'epoch': 0.43}\n",
      "{'loss': 2.793, 'grad_norm': 4.695980548858643, 'learning_rate': 9.090909090909091e-06, 'epoch': 0.52}\n",
      "{'loss': 2.9096, 'grad_norm': 3.711344003677368, 'learning_rate': 1.0606060606060607e-05, 'epoch': 0.61}\n",
      "{'loss': 2.88, 'grad_norm': 3.3542118072509766, 'learning_rate': 1.2121212121212122e-05, 'epoch': 0.7}\n",
      "{'loss': 2.8675, 'grad_norm': 3.893371820449829, 'learning_rate': 1.3636363636363637e-05, 'epoch': 0.78}\n",
      "{'loss': 2.5508, 'grad_norm': 3.101470708847046, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.87}\n",
      "{'loss': 2.7004, 'grad_norm': 3.076831579208374, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.96}\n",
      "{'loss': 2.5563, 'grad_norm': 3.208193063735962, 'learning_rate': 1.8181818181818182e-05, 'epoch': 1.04}\n",
      "{'loss': 2.5038, 'grad_norm': 2.705632209777832, 'learning_rate': 1.9696969696969697e-05, 'epoch': 1.13}\n",
      "{'loss': 2.5226, 'grad_norm': 2.7284839153289795, 'learning_rate': 2.1212121212121215e-05, 'epoch': 1.22}\n",
      "{'loss': 2.4895, 'grad_norm': 2.657764196395874, 'learning_rate': 2.272727272727273e-05, 'epoch': 1.3}\n",
      "{'loss': 2.4771, 'grad_norm': 2.7615675926208496, 'learning_rate': 2.4242424242424244e-05, 'epoch': 1.39}\n",
      "{'loss': 2.1594, 'grad_norm': 2.979644298553467, 'learning_rate': 2.575757575757576e-05, 'epoch': 1.48}\n",
      "{'loss': 2.3542, 'grad_norm': 2.455160140991211, 'learning_rate': 2.7272727272727273e-05, 'epoch': 1.57}\n",
      "{'loss': 2.1256, 'grad_norm': 2.563417434692383, 'learning_rate': 2.878787878787879e-05, 'epoch': 1.65}\n",
      "{'loss': 2.205, 'grad_norm': 2.3794631958007812, 'learning_rate': 3.0303030303030306e-05, 'epoch': 1.74}\n",
      "{'loss': 2.1195, 'grad_norm': 2.2835042476654053, 'learning_rate': 3.181818181818182e-05, 'epoch': 1.83}\n",
      "{'loss': 2.1, 'grad_norm': 2.2119710445404053, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.91}\n",
      "{'loss': 2.094, 'grad_norm': 2.9570953845977783, 'learning_rate': 3.484848484848485e-05, 'epoch': 2.0}\n",
      "{'loss': 1.9463, 'grad_norm': 2.0305252075195312, 'learning_rate': 3.6363636363636364e-05, 'epoch': 2.09}\n",
      "{'loss': 1.6584, 'grad_norm': 2.9757943153381348, 'learning_rate': 3.787878787878788e-05, 'epoch': 2.17}\n",
      "{'loss': 1.7976, 'grad_norm': 2.9868502616882324, 'learning_rate': 3.939393939393939e-05, 'epoch': 2.26}\n",
      "{'loss': 1.935, 'grad_norm': 2.1592888832092285, 'learning_rate': 4.0909090909090915e-05, 'epoch': 2.35}\n",
      "{'loss': 1.7143, 'grad_norm': 2.3868980407714844, 'learning_rate': 4.242424242424243e-05, 'epoch': 2.43}\n",
      "{'loss': 1.8001, 'grad_norm': 2.2970337867736816, 'learning_rate': 4.3939393939393944e-05, 'epoch': 2.52}\n",
      "{'loss': 1.838, 'grad_norm': 2.1893203258514404, 'learning_rate': 4.545454545454546e-05, 'epoch': 2.61}\n",
      "{'loss': 1.7356, 'grad_norm': 2.3592114448547363, 'learning_rate': 4.696969696969697e-05, 'epoch': 2.7}\n",
      "{'loss': 1.7787, 'grad_norm': 2.3354921340942383, 'learning_rate': 4.848484848484849e-05, 'epoch': 2.78}\n",
      "{'loss': 1.6653, 'grad_norm': 2.3432133197784424, 'learning_rate': 5e-05, 'epoch': 2.87}\n",
      "{'loss': 1.6792, 'grad_norm': 2.1168718338012695, 'learning_rate': 4.962121212121213e-05, 'epoch': 2.96}\n",
      "{'loss': 1.4229, 'grad_norm': 2.417908191680908, 'learning_rate': 4.9242424242424245e-05, 'epoch': 3.04}\n",
      "{'loss': 1.3733, 'grad_norm': 2.1560630798339844, 'learning_rate': 4.886363636363637e-05, 'epoch': 3.13}\n",
      "{'loss': 1.4976, 'grad_norm': 2.3467559814453125, 'learning_rate': 4.848484848484849e-05, 'epoch': 3.22}\n",
      "{'loss': 1.4141, 'grad_norm': 2.689695119857788, 'learning_rate': 4.810606060606061e-05, 'epoch': 3.3}\n",
      "{'loss': 1.2436, 'grad_norm': 2.739992141723633, 'learning_rate': 4.772727272727273e-05, 'epoch': 3.39}\n",
      "{'loss': 1.2345, 'grad_norm': 2.326427698135376, 'learning_rate': 4.7348484848484855e-05, 'epoch': 3.48}\n",
      "{'loss': 1.3085, 'grad_norm': 2.398743152618408, 'learning_rate': 4.696969696969697e-05, 'epoch': 3.57}\n",
      "{'loss': 1.4027, 'grad_norm': 2.4807064533233643, 'learning_rate': 4.659090909090909e-05, 'epoch': 3.65}\n",
      "{'loss': 1.1751, 'grad_norm': 2.776124954223633, 'learning_rate': 4.621212121212121e-05, 'epoch': 3.74}\n",
      "{'loss': 1.3622, 'grad_norm': 2.5062613487243652, 'learning_rate': 4.5833333333333334e-05, 'epoch': 3.83}\n",
      "{'loss': 1.1644, 'grad_norm': 2.384096384048462, 'learning_rate': 4.545454545454546e-05, 'epoch': 3.91}\n",
      "{'loss': 1.1399, 'grad_norm': 2.5821189880371094, 'learning_rate': 4.5075757575757577e-05, 'epoch': 4.0}\n",
      "{'loss': 0.7998, 'grad_norm': 2.657325267791748, 'learning_rate': 4.46969696969697e-05, 'epoch': 4.09}\n",
      "{'loss': 0.9279, 'grad_norm': 2.8054521083831787, 'learning_rate': 4.431818181818182e-05, 'epoch': 4.17}\n",
      "{'loss': 0.9283, 'grad_norm': 3.845602512359619, 'learning_rate': 4.3939393939393944e-05, 'epoch': 4.26}\n",
      "{'loss': 0.9448, 'grad_norm': 5.698088645935059, 'learning_rate': 4.356060606060606e-05, 'epoch': 4.35}\n",
      "{'loss': 0.7785, 'grad_norm': 2.859131097793579, 'learning_rate': 4.318181818181819e-05, 'epoch': 4.43}\n",
      "{'loss': 0.9013, 'grad_norm': 2.8174407482147217, 'learning_rate': 4.2803030303030305e-05, 'epoch': 4.52}\n",
      "{'loss': 0.8273, 'grad_norm': 2.785587787628174, 'learning_rate': 4.242424242424243e-05, 'epoch': 4.61}\n",
      "{'loss': 0.9876, 'grad_norm': 2.962296724319458, 'learning_rate': 4.204545454545455e-05, 'epoch': 4.7}\n",
      "{'loss': 0.8588, 'grad_norm': 3.1537256240844727, 'learning_rate': 4.166666666666667e-05, 'epoch': 4.78}\n",
      "{'loss': 0.7287, 'grad_norm': 3.5033047199249268, 'learning_rate': 4.128787878787879e-05, 'epoch': 4.87}\n",
      "{'loss': 0.8051, 'grad_norm': 3.8486409187316895, 'learning_rate': 4.0909090909090915e-05, 'epoch': 4.96}\n",
      "{'loss': 0.694, 'grad_norm': 3.25285267829895, 'learning_rate': 4.053030303030303e-05, 'epoch': 5.04}\n",
      "{'loss': 0.5602, 'grad_norm': 3.0240297317504883, 'learning_rate': 4.015151515151515e-05, 'epoch': 5.13}\n",
      "{'loss': 0.5307, 'grad_norm': 2.9190421104431152, 'learning_rate': 3.9772727272727275e-05, 'epoch': 5.22}\n",
      "{'loss': 0.5605, 'grad_norm': 3.135488986968994, 'learning_rate': 3.939393939393939e-05, 'epoch': 5.3}\n",
      "{'loss': 0.5498, 'grad_norm': 3.9967846870422363, 'learning_rate': 3.901515151515152e-05, 'epoch': 5.39}\n",
      "{'loss': 0.4642, 'grad_norm': 4.26421594619751, 'learning_rate': 3.8636363636363636e-05, 'epoch': 5.48}\n",
      "{'loss': 0.5405, 'grad_norm': 4.014067649841309, 'learning_rate': 3.825757575757576e-05, 'epoch': 5.57}\n",
      "{'loss': 0.5123, 'grad_norm': 3.503504991531372, 'learning_rate': 3.787878787878788e-05, 'epoch': 5.65}\n",
      "{'loss': 0.4526, 'grad_norm': 3.686959981918335, 'learning_rate': 3.7500000000000003e-05, 'epoch': 5.74}\n",
      "{'loss': 0.4882, 'grad_norm': 3.435720443725586, 'learning_rate': 3.712121212121212e-05, 'epoch': 5.83}\n",
      "{'loss': 0.4658, 'grad_norm': 3.496185302734375, 'learning_rate': 3.6742424242424246e-05, 'epoch': 5.91}\n",
      "{'loss': 0.4656, 'grad_norm': 3.478749990463257, 'learning_rate': 3.6363636363636364e-05, 'epoch': 6.0}\n",
      "{'loss': 0.3382, 'grad_norm': 3.1103250980377197, 'learning_rate': 3.598484848484849e-05, 'epoch': 6.09}\n",
      "{'loss': 0.2634, 'grad_norm': 3.982386589050293, 'learning_rate': 3.560606060606061e-05, 'epoch': 6.17}\n",
      "{'loss': 0.2662, 'grad_norm': 3.8359858989715576, 'learning_rate': 3.522727272727273e-05, 'epoch': 6.26}\n",
      "{'loss': 0.2492, 'grad_norm': 3.30759334564209, 'learning_rate': 3.484848484848485e-05, 'epoch': 6.35}\n",
      "{'loss': 0.3019, 'grad_norm': 4.478699684143066, 'learning_rate': 3.4469696969696974e-05, 'epoch': 6.43}\n",
      "{'loss': 0.2576, 'grad_norm': 3.1405720710754395, 'learning_rate': 3.409090909090909e-05, 'epoch': 6.52}\n",
      "{'loss': 0.2392, 'grad_norm': 3.4898905754089355, 'learning_rate': 3.371212121212121e-05, 'epoch': 6.61}\n",
      "{'loss': 0.287, 'grad_norm': 3.4276742935180664, 'learning_rate': 3.3333333333333335e-05, 'epoch': 6.7}\n",
      "{'loss': 0.2595, 'grad_norm': 3.201711893081665, 'learning_rate': 3.295454545454545e-05, 'epoch': 6.78}\n",
      "{'loss': 0.1723, 'grad_norm': 2.5373401641845703, 'learning_rate': 3.257575757575758e-05, 'epoch': 6.87}\n",
      "{'loss': 0.2149, 'grad_norm': 10.8377103805542, 'learning_rate': 3.2196969696969696e-05, 'epoch': 6.96}\n",
      "{'loss': 0.203, 'grad_norm': 2.83239483833313, 'learning_rate': 3.181818181818182e-05, 'epoch': 7.04}\n",
      "{'loss': 0.1414, 'grad_norm': 2.3952155113220215, 'learning_rate': 3.143939393939394e-05, 'epoch': 7.13}\n",
      "{'loss': 0.1655, 'grad_norm': 3.1169235706329346, 'learning_rate': 3.106060606060606e-05, 'epoch': 7.22}\n",
      "{'loss': 0.1427, 'grad_norm': 2.663719654083252, 'learning_rate': 3.068181818181818e-05, 'epoch': 7.3}\n",
      "{'loss': 0.1239, 'grad_norm': 3.178455114364624, 'learning_rate': 3.0303030303030306e-05, 'epoch': 7.39}\n",
      "{'loss': 0.113, 'grad_norm': 2.6828606128692627, 'learning_rate': 2.9924242424242427e-05, 'epoch': 7.48}\n",
      "{'loss': 0.1384, 'grad_norm': 3.0284152030944824, 'learning_rate': 2.954545454545455e-05, 'epoch': 7.57}\n",
      "{'loss': 0.1189, 'grad_norm': 2.6847684383392334, 'learning_rate': 2.916666666666667e-05, 'epoch': 7.65}\n",
      "{'loss': 0.1507, 'grad_norm': 2.8911049365997314, 'learning_rate': 2.878787878787879e-05, 'epoch': 7.74}\n",
      "{'loss': 0.1165, 'grad_norm': 2.383119821548462, 'learning_rate': 2.8409090909090912e-05, 'epoch': 7.83}\n",
      "{'loss': 0.1431, 'grad_norm': 2.5005416870117188, 'learning_rate': 2.803030303030303e-05, 'epoch': 7.91}\n",
      "{'loss': 0.1237, 'grad_norm': 2.350477457046509, 'learning_rate': 2.7651515151515152e-05, 'epoch': 8.0}\n",
      "{'loss': 0.0764, 'grad_norm': 1.6819177865982056, 'learning_rate': 2.7272727272727273e-05, 'epoch': 8.09}\n",
      "{'loss': 0.0715, 'grad_norm': 1.783036470413208, 'learning_rate': 2.6893939393939394e-05, 'epoch': 8.17}\n",
      "{'loss': 0.066, 'grad_norm': 1.8136919736862183, 'learning_rate': 2.6515151515151516e-05, 'epoch': 8.26}\n",
      "{'loss': 0.0694, 'grad_norm': 2.254955530166626, 'learning_rate': 2.6136363636363637e-05, 'epoch': 8.35}\n",
      "{'loss': 0.0756, 'grad_norm': 2.448960304260254, 'learning_rate': 2.575757575757576e-05, 'epoch': 8.43}\n",
      "{'loss': 0.0673, 'grad_norm': 2.318617582321167, 'learning_rate': 2.537878787878788e-05, 'epoch': 8.52}\n",
      "{'loss': 0.0845, 'grad_norm': 2.484612464904785, 'learning_rate': 2.5e-05, 'epoch': 8.61}\n",
      "{'loss': 0.0728, 'grad_norm': 2.0480635166168213, 'learning_rate': 2.4621212121212123e-05, 'epoch': 8.7}\n",
      "{'loss': 0.0666, 'grad_norm': 1.8201649188995361, 'learning_rate': 2.4242424242424244e-05, 'epoch': 8.78}\n",
      "{'loss': 0.0688, 'grad_norm': 2.1162190437316895, 'learning_rate': 2.3863636363636365e-05, 'epoch': 8.87}\n",
      "{'loss': 0.0844, 'grad_norm': 2.0863735675811768, 'learning_rate': 2.3484848484848487e-05, 'epoch': 8.96}\n",
      "{'loss': 0.0653, 'grad_norm': 2.160491943359375, 'learning_rate': 2.3106060606060605e-05, 'epoch': 9.04}\n",
      "{'loss': 0.0454, 'grad_norm': 1.2886521816253662, 'learning_rate': 2.272727272727273e-05, 'epoch': 9.13}\n",
      "{'loss': 0.0431, 'grad_norm': 1.3262633085250854, 'learning_rate': 2.234848484848485e-05, 'epoch': 9.22}\n",
      "{'loss': 0.062, 'grad_norm': 2.6298060417175293, 'learning_rate': 2.1969696969696972e-05, 'epoch': 9.3}\n",
      "{'loss': 0.0523, 'grad_norm': 1.9874898195266724, 'learning_rate': 2.1590909090909093e-05, 'epoch': 9.39}\n",
      "{'loss': 0.0565, 'grad_norm': 2.379868984222412, 'learning_rate': 2.1212121212121215e-05, 'epoch': 9.48}\n",
      "{'loss': 0.0595, 'grad_norm': 2.1696579456329346, 'learning_rate': 2.0833333333333336e-05, 'epoch': 9.57}\n",
      "{'loss': 0.0559, 'grad_norm': 1.996274709701538, 'learning_rate': 2.0454545454545457e-05, 'epoch': 9.65}\n",
      "{'loss': 0.059, 'grad_norm': 2.186664342880249, 'learning_rate': 2.0075757575757575e-05, 'epoch': 9.74}\n",
      "{'loss': 0.0559, 'grad_norm': 1.8140538930892944, 'learning_rate': 1.9696969696969697e-05, 'epoch': 9.83}\n",
      "{'loss': 0.0528, 'grad_norm': 1.7162374258041382, 'learning_rate': 1.9318181818181818e-05, 'epoch': 9.91}\n",
      "{'loss': 0.0656, 'grad_norm': 2.5798559188842773, 'learning_rate': 1.893939393939394e-05, 'epoch': 10.0}\n",
      "{'loss': 0.0376, 'grad_norm': 1.07387113571167, 'learning_rate': 1.856060606060606e-05, 'epoch': 10.09}\n",
      "{'loss': 0.0332, 'grad_norm': 0.9945735931396484, 'learning_rate': 1.8181818181818182e-05, 'epoch': 10.17}\n",
      "{'loss': 0.0423, 'grad_norm': 1.8461298942565918, 'learning_rate': 1.7803030303030303e-05, 'epoch': 10.26}\n",
      "{'loss': 0.0368, 'grad_norm': 1.2583087682724, 'learning_rate': 1.7424242424242425e-05, 'epoch': 10.35}\n",
      "{'loss': 0.0369, 'grad_norm': 1.266119360923767, 'learning_rate': 1.7045454545454546e-05, 'epoch': 10.43}\n",
      "{'loss': 0.0331, 'grad_norm': 1.493351697921753, 'learning_rate': 1.6666666666666667e-05, 'epoch': 10.52}\n",
      "{'loss': 0.0366, 'grad_norm': 1.5610707998275757, 'learning_rate': 1.628787878787879e-05, 'epoch': 10.61}\n",
      "{'loss': 0.0435, 'grad_norm': 1.5294173955917358, 'learning_rate': 1.590909090909091e-05, 'epoch': 10.7}\n",
      "{'loss': 0.0428, 'grad_norm': 1.405829668045044, 'learning_rate': 1.553030303030303e-05, 'epoch': 10.78}\n",
      "{'loss': 0.0352, 'grad_norm': 1.5450570583343506, 'learning_rate': 1.5151515151515153e-05, 'epoch': 10.87}\n",
      "{'loss': 0.0386, 'grad_norm': 1.392519474029541, 'learning_rate': 1.4772727272727274e-05, 'epoch': 10.96}\n",
      "{'loss': 0.032, 'grad_norm': 0.9445621371269226, 'learning_rate': 1.4393939393939396e-05, 'epoch': 11.04}\n",
      "{'loss': 0.0248, 'grad_norm': 0.6936696171760559, 'learning_rate': 1.4015151515151515e-05, 'epoch': 11.13}\n",
      "{'loss': 0.023, 'grad_norm': 0.6500676274299622, 'learning_rate': 1.3636363636363637e-05, 'epoch': 11.22}\n",
      "{'loss': 0.0238, 'grad_norm': 1.035893440246582, 'learning_rate': 1.3257575757575758e-05, 'epoch': 11.3}\n",
      "{'loss': 0.0322, 'grad_norm': 1.0040099620819092, 'learning_rate': 1.287878787878788e-05, 'epoch': 11.39}\n",
      "{'loss': 0.0308, 'grad_norm': 1.1131609678268433, 'learning_rate': 1.25e-05, 'epoch': 11.48}\n",
      "{'loss': 0.0292, 'grad_norm': 0.9462449550628662, 'learning_rate': 1.2121212121212122e-05, 'epoch': 11.57}\n",
      "{'loss': 0.0257, 'grad_norm': 0.7351125478744507, 'learning_rate': 1.1742424242424243e-05, 'epoch': 11.65}\n",
      "{'loss': 0.0262, 'grad_norm': 1.3590242862701416, 'learning_rate': 1.1363636363636365e-05, 'epoch': 11.74}\n",
      "{'loss': 0.0306, 'grad_norm': 0.9796245098114014, 'learning_rate': 1.0984848484848486e-05, 'epoch': 11.83}\n",
      "{'loss': 0.0272, 'grad_norm': 0.826877236366272, 'learning_rate': 1.0606060606060607e-05, 'epoch': 11.91}\n",
      "{'loss': 0.0573, 'grad_norm': 2.4185802936553955, 'learning_rate': 1.0227272727272729e-05, 'epoch': 12.0}\n",
      "{'loss': 0.0219, 'grad_norm': 0.504853367805481, 'learning_rate': 9.848484848484848e-06, 'epoch': 12.09}\n",
      "{'loss': 0.026, 'grad_norm': 0.7431012392044067, 'learning_rate': 9.46969696969697e-06, 'epoch': 12.17}\n",
      "{'loss': 0.021, 'grad_norm': 0.5128331184387207, 'learning_rate': 9.090909090909091e-06, 'epoch': 12.26}\n",
      "{'loss': 0.0219, 'grad_norm': 0.5809022188186646, 'learning_rate': 8.712121212121212e-06, 'epoch': 12.35}\n",
      "{'loss': 0.0218, 'grad_norm': 0.6436299681663513, 'learning_rate': 8.333333333333334e-06, 'epoch': 12.43}\n",
      "{'loss': 0.0256, 'grad_norm': 1.069245457649231, 'learning_rate': 7.954545454545455e-06, 'epoch': 12.52}\n",
      "{'loss': 0.0272, 'grad_norm': 0.8052883148193359, 'learning_rate': 7.5757575757575764e-06, 'epoch': 12.61}\n",
      "{'loss': 0.0274, 'grad_norm': 0.8671571016311646, 'learning_rate': 7.196969696969698e-06, 'epoch': 12.7}\n",
      "{'loss': 0.0212, 'grad_norm': 0.7426294684410095, 'learning_rate': 6.818181818181818e-06, 'epoch': 12.78}\n",
      "{'loss': 0.0242, 'grad_norm': 0.6701984405517578, 'learning_rate': 6.43939393939394e-06, 'epoch': 12.87}\n",
      "{'loss': 0.0232, 'grad_norm': 0.6567612290382385, 'learning_rate': 6.060606060606061e-06, 'epoch': 12.96}\n",
      "{'loss': 0.0352, 'grad_norm': 0.8136070370674133, 'learning_rate': 5.681818181818182e-06, 'epoch': 13.04}\n",
      "{'loss': 0.0196, 'grad_norm': 0.48340338468551636, 'learning_rate': 5.303030303030304e-06, 'epoch': 13.13}\n",
      "{'loss': 0.021, 'grad_norm': 0.48410192131996155, 'learning_rate': 4.924242424242424e-06, 'epoch': 13.22}\n",
      "{'loss': 0.0174, 'grad_norm': 0.35442960262298584, 'learning_rate': 4.5454545454545455e-06, 'epoch': 13.3}\n",
      "{'loss': 0.0193, 'grad_norm': 0.3440489172935486, 'learning_rate': 4.166666666666667e-06, 'epoch': 13.39}\n",
      "{'loss': 0.0238, 'grad_norm': 0.45886915922164917, 'learning_rate': 3.7878787878787882e-06, 'epoch': 13.48}\n",
      "{'loss': 0.0211, 'grad_norm': 0.5850061178207397, 'learning_rate': 3.409090909090909e-06, 'epoch': 13.57}\n",
      "{'loss': 0.0168, 'grad_norm': 0.43199875950813293, 'learning_rate': 3.0303030303030305e-06, 'epoch': 13.65}\n",
      "{'loss': 0.0193, 'grad_norm': 0.4194183945655823, 'learning_rate': 2.651515151515152e-06, 'epoch': 13.74}\n",
      "{'loss': 0.0163, 'grad_norm': 0.3619418144226074, 'learning_rate': 2.2727272727272728e-06, 'epoch': 13.83}\n",
      "{'loss': 0.0183, 'grad_norm': 0.4890177845954895, 'learning_rate': 1.8939393939393941e-06, 'epoch': 13.91}\n",
      "{'loss': 0.0227, 'grad_norm': 1.175419569015503, 'learning_rate': 1.5151515151515152e-06, 'epoch': 14.0}\n",
      "{'loss': 0.0163, 'grad_norm': 0.2957412898540497, 'learning_rate': 1.1363636363636364e-06, 'epoch': 14.09}\n",
      "{'loss': 0.019, 'grad_norm': 0.3509805202484131, 'learning_rate': 7.575757575757576e-07, 'epoch': 14.17}\n",
      "{'loss': 0.0158, 'grad_norm': 0.3104557394981384, 'learning_rate': 3.787878787878788e-07, 'epoch': 14.26}\n",
      "{'loss': 0.0161, 'grad_norm': 0.31688329577445984, 'learning_rate': 0.0, 'epoch': 14.35}\n",
      "{'train_runtime': 658.817, 'train_samples_per_second': 4.189, 'train_steps_per_second': 0.25, 'train_loss': 0.7196285456193215, 'epoch': 14.35}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how much memory was used.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090. Max memory = 23.668 GB.\n",
      "20.855 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What is John Carter's Sandbox "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy?\n",
      "\n",
      "By: \n",
      "John Carter\n",
      "\n",
      "Available on: \n",
      "ThinkorSwim, TradingView\n",
      "\n",
      "This product is designed for: \n",
      "Options, Futures, Stocks\n",
      "\n",
      "Trading in any market successfully requires a trading plan. The best trading plans are designed to help traders to identify high-probability setups while protecting their profits. In this class, John Carter, Director of Options and Futures, will share his “Sandbox Strategy” which he’s used to reliably day trade the Newhouse 3 (NH3) index within a pre-defined “sandbox.”\n",
      "\n",
      "Each trading day begins with an interactive session where John and NH3 traders come together\n"
     ]
    }
   ],
   "source": [
    "# Enable native faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Define the question\n",
    "question = \"What is John Carter's Sandbox Strategy?\"\n",
    "\n",
    "# Format the input\n",
    "formatted_input = question\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    [formatted_input],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the output using the model\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ready to save model -- input your own huggingface or local variables\n",
    "- Save a base model to keep training and quantized formats for testing inference locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"davidbzyk\"\n",
    "base_model_name = \"simpler-gemma-2-2b\"\n",
    "base_repo=f\"{username}/{base_model_name}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d251722b584058b4867c32b471b778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/5.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/simpler-gemma-2-2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d9d6ca732e4659936759c3e99e26a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaadddd20b1948588bacf6e1f776600e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4476efc75a0d4dd59f07d9404b84a9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(base_model_name, token=HUGGING_FACE_HUB_TOKEN)\n",
    "tokenizer.push_to_hub(base_model_name, token=HUGGING_FACE_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_models_name = \"simpler-gemma-2-2b-multi\"\n",
    "multi_repo=f\"{username}/{multi_models_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 88.52 out of 125.61 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 115.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting gemma2 model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at simpler-gemma-2-2b-multi into bf16 GGUF format.\n",
      "The output location will be ./simpler-gemma-2-2b-multi/unsloth.BF16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: simpler-gemma-2-2b-multi\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> BF16, shape = {2304, 256000}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.bfloat16 --> BF16, shape = {9216, 2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.bfloat16 --> BF16, shape = {2304, 9216}\n",
      "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.bfloat16 --> BF16, shape = {2048, 2304}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.bfloat16 --> BF16, shape = {2304, 2048}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.bfloat16 --> BF16, shape = {2304, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2304}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 2\n",
      "INFO:gguf.vocab:Setting special token type eos to 1\n",
      "INFO:gguf.vocab:Setting special token type unk to 3\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:simpler-gemma-2-2b-multi/unsloth.BF16.gguf: n_tensors = 288, total_size = 5.2G\n",
      "Writing: 100%|██████████| 5.23G/5.23G [00:09<00:00, 561Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to simpler-gemma-2-2b-multi/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: ./simpler-gemma-2-2b-multi/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3509 (ecf6b7f2)\n",
      "main: built with x86_64-conda-linux-gnu-cc (conda-forge gcc 12.4.0-0) 12.4.0 for x86_64-conda-linux-gnu\n",
      "main: quantizing './simpler-gemma-2-2b-multi/unsloth.BF16.gguf' to './simpler-gemma-2-2b-multi/unsloth.Q4_K_M.gguf' as Q4_K_M using 64 threads\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 288 tensors from ./simpler-gemma-2-2b-multi/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = it-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type bf16:  183 tensors\n",
      "[   1/ 288]                    token_embd.weight - [ 2304, 256000,     1,     1], type =   bf16, converting to q6_K .. size =  1125.00 MiB ->   461.43 MiB\n",
      "[   2/ 288]               blk.0.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   3/ 288]                blk.0.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[   4/ 288]                blk.0.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[   5/ 288]                  blk.0.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[   6/ 288]     blk.0.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   7/ 288]           blk.0.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   8/ 288]                blk.0.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   9/ 288]                  blk.0.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  10/ 288]             blk.0.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  11/ 288]                  blk.0.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  12/ 288]                  blk.0.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  13/ 288]               blk.1.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  14/ 288]                blk.1.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  15/ 288]                blk.1.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  16/ 288]                  blk.1.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  17/ 288]     blk.1.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  18/ 288]           blk.1.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  19/ 288]                blk.1.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  20/ 288]                  blk.1.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  21/ 288]             blk.1.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  22/ 288]                  blk.1.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  23/ 288]                  blk.1.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  24/ 288]              blk.10.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  25/ 288]               blk.10.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  26/ 288]               blk.10.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  27/ 288]                 blk.10.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  28/ 288]    blk.10.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  29/ 288]          blk.10.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  30/ 288]               blk.10.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  31/ 288]                 blk.10.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  32/ 288]            blk.10.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  33/ 288]                 blk.10.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  34/ 288]                 blk.10.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  35/ 288]              blk.11.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  36/ 288]               blk.11.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  37/ 288]               blk.11.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  38/ 288]                 blk.11.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  39/ 288]    blk.11.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  40/ 288]          blk.11.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  41/ 288]               blk.11.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  42/ 288]                 blk.11.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  43/ 288]            blk.11.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  44/ 288]                 blk.11.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  45/ 288]                 blk.11.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  46/ 288]              blk.12.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  47/ 288]               blk.12.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  48/ 288]               blk.12.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  49/ 288]                 blk.12.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  50/ 288]    blk.12.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  51/ 288]          blk.12.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  52/ 288]               blk.12.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  53/ 288]                 blk.12.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  54/ 288]            blk.12.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  55/ 288]                 blk.12.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  56/ 288]                 blk.12.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  57/ 288]              blk.13.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  58/ 288]               blk.13.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  59/ 288]               blk.13.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  60/ 288]                 blk.13.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  61/ 288]    blk.13.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  62/ 288]          blk.13.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  63/ 288]               blk.13.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  64/ 288]                 blk.13.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  65/ 288]            blk.13.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  66/ 288]                 blk.13.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  67/ 288]                 blk.13.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  68/ 288]              blk.14.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  69/ 288]               blk.14.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  70/ 288]               blk.14.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  71/ 288]                 blk.14.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  72/ 288]    blk.14.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  73/ 288]          blk.14.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  74/ 288]               blk.14.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  75/ 288]                 blk.14.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  76/ 288]            blk.14.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  77/ 288]                 blk.14.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  78/ 288]                 blk.14.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  79/ 288]              blk.15.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  80/ 288]               blk.15.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  81/ 288]               blk.15.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  82/ 288]                 blk.15.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  83/ 288]    blk.15.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  84/ 288]          blk.15.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  85/ 288]               blk.15.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  86/ 288]                 blk.15.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  87/ 288]            blk.15.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  88/ 288]                 blk.15.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  89/ 288]                 blk.15.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  90/ 288]              blk.16.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  91/ 288]               blk.16.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  92/ 288]               blk.16.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  93/ 288]                 blk.16.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[  94/ 288]    blk.16.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  95/ 288]          blk.16.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  96/ 288]               blk.16.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  97/ 288]                 blk.16.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[  98/ 288]            blk.16.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[  99/ 288]                 blk.16.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 100/ 288]                 blk.16.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 101/ 288]              blk.17.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 102/ 288]               blk.17.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 103/ 288]               blk.17.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 104/ 288]                 blk.17.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 105/ 288]    blk.17.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 106/ 288]          blk.17.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 107/ 288]               blk.17.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 108/ 288]                 blk.17.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 109/ 288]            blk.17.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 110/ 288]                 blk.17.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 111/ 288]                 blk.17.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 112/ 288]              blk.18.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 113/ 288]               blk.18.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 114/ 288]               blk.18.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 115/ 288]                 blk.18.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 116/ 288]    blk.18.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 117/ 288]          blk.18.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 118/ 288]               blk.18.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 119/ 288]                 blk.18.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 120/ 288]            blk.18.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 121/ 288]                 blk.18.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 122/ 288]                 blk.18.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 123/ 288]              blk.19.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 124/ 288]               blk.19.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 125/ 288]               blk.19.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 126/ 288]                 blk.19.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 127/ 288]    blk.19.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 128/ 288]          blk.19.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 129/ 288]               blk.19.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 130/ 288]                 blk.19.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 131/ 288]            blk.19.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 132/ 288]                 blk.19.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 133/ 288]                 blk.19.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 134/ 288]               blk.2.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 135/ 288]                blk.2.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 136/ 288]                blk.2.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 137/ 288]                  blk.2.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 138/ 288]     blk.2.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 139/ 288]           blk.2.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 140/ 288]                blk.2.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 141/ 288]                  blk.2.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 142/ 288]             blk.2.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 143/ 288]                  blk.2.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 144/ 288]                  blk.2.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 145/ 288]              blk.20.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 146/ 288]               blk.20.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 147/ 288]               blk.20.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 148/ 288]                 blk.20.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 149/ 288]    blk.20.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 150/ 288]          blk.20.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 151/ 288]               blk.20.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 152/ 288]                 blk.20.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 153/ 288]            blk.20.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 154/ 288]                 blk.20.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 155/ 288]                 blk.20.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 156/ 288]              blk.21.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 157/ 288]               blk.21.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 158/ 288]               blk.21.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 159/ 288]                 blk.21.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 160/ 288]    blk.21.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 161/ 288]          blk.21.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 162/ 288]               blk.21.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 163/ 288]                 blk.21.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 164/ 288]            blk.21.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 165/ 288]                 blk.21.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 166/ 288]                 blk.21.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 167/ 288]              blk.22.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 168/ 288]               blk.22.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 169/ 288]               blk.22.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 170/ 288]                 blk.22.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 171/ 288]    blk.22.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 172/ 288]          blk.22.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 173/ 288]               blk.22.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 174/ 288]                 blk.22.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 175/ 288]            blk.22.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 176/ 288]                 blk.22.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 177/ 288]                 blk.22.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 178/ 288]              blk.23.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 179/ 288]               blk.23.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 180/ 288]               blk.23.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 181/ 288]                 blk.23.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 182/ 288]    blk.23.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 183/ 288]          blk.23.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 184/ 288]               blk.23.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 185/ 288]                 blk.23.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 186/ 288]            blk.23.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 187/ 288]                 blk.23.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 188/ 288]                 blk.23.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 189/ 288]               blk.24.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 190/ 288]                 blk.24.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 191/ 288]            blk.24.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 192/ 288]                 blk.24.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 193/ 288]                 blk.24.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 194/ 288]               blk.3.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 195/ 288]                blk.3.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 196/ 288]                blk.3.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 197/ 288]                  blk.3.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 198/ 288]     blk.3.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 199/ 288]           blk.3.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 200/ 288]                blk.3.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 201/ 288]                  blk.3.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 202/ 288]             blk.3.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 203/ 288]                  blk.3.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 204/ 288]                  blk.3.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 205/ 288]               blk.4.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 206/ 288]                blk.4.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 207/ 288]                blk.4.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 208/ 288]                  blk.4.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 209/ 288]     blk.4.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 210/ 288]           blk.4.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 211/ 288]                blk.4.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 212/ 288]                  blk.4.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 213/ 288]             blk.4.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 214/ 288]                  blk.4.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 215/ 288]                  blk.4.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 216/ 288]               blk.5.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 217/ 288]                blk.5.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 218/ 288]                blk.5.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 219/ 288]                  blk.5.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 220/ 288]     blk.5.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 221/ 288]           blk.5.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 222/ 288]                blk.5.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 223/ 288]                  blk.5.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 224/ 288]             blk.5.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 225/ 288]                  blk.5.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 226/ 288]                  blk.5.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 227/ 288]               blk.6.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 228/ 288]                blk.6.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 229/ 288]                blk.6.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 230/ 288]                  blk.6.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 231/ 288]     blk.6.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 232/ 288]           blk.6.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 233/ 288]                blk.6.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 234/ 288]                  blk.6.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 235/ 288]             blk.6.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 236/ 288]                  blk.6.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 237/ 288]                  blk.6.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 238/ 288]               blk.7.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 239/ 288]                blk.7.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 240/ 288]                blk.7.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 241/ 288]                  blk.7.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 242/ 288]     blk.7.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 243/ 288]           blk.7.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 244/ 288]                blk.7.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 245/ 288]                  blk.7.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 246/ 288]             blk.7.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 247/ 288]                  blk.7.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 248/ 288]                  blk.7.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 249/ 288]               blk.8.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 250/ 288]                blk.8.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 251/ 288]                blk.8.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 252/ 288]                  blk.8.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 253/ 288]     blk.8.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 254/ 288]           blk.8.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 255/ 288]                blk.8.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 256/ 288]                  blk.8.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 257/ 288]             blk.8.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 258/ 288]                  blk.8.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 259/ 288]                  blk.8.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 260/ 288]               blk.9.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 261/ 288]                blk.9.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 262/ 288]                blk.9.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 263/ 288]                  blk.9.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 264/ 288]     blk.9.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 265/ 288]           blk.9.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 266/ 288]                blk.9.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 267/ 288]                  blk.9.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 268/ 288]             blk.9.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 269/ 288]                  blk.9.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 270/ 288]                  blk.9.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 271/ 288]              blk.24.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 272/ 288]               blk.24.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 273/ 288]                 blk.24.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 274/ 288]    blk.24.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 275/ 288]          blk.24.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 276/ 288]               blk.24.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 277/ 288]              blk.25.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 278/ 288]               blk.25.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 279/ 288]               blk.25.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 280/ 288]                 blk.25.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q4_K .. size =    40.50 MiB ->    11.39 MiB\n",
      "[ 281/ 288]    blk.25.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 282/ 288]          blk.25.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 283/ 288]               blk.25.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 284/ 288]                 blk.25.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
      "[ 285/ 288]            blk.25.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 286/ 288]                 blk.25.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     9.00 MiB ->     2.53 MiB\n",
      "[ 287/ 288]                 blk.25.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 288/ 288]                   output_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "llama_model_quantize_internal: model size  =  4986.92 MB\n",
      "llama_model_quantize_internal: quant size  =  1623.67 MB\n",
      "\n",
      "main: quantize time =  7973.03 ms\n",
      "main:    total time =  7973.03 ms\n",
      "Unsloth: Conversion completed! Output location: ./simpler-gemma-2-2b-multi/unsloth.Q4_K_M.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\n",
      "main: build = 3509 (ecf6b7f2)\n",
      "main: built with x86_64-conda-linux-gnu-cc (conda-forge gcc 12.4.0-0) 12.4.0 for x86_64-conda-linux-gnu\n",
      "main: quantizing './simpler-gemma-2-2b-multi/unsloth.BF16.gguf' to './simpler-gemma-2-2b-multi/unsloth.Q8_0.gguf' as Q8_0 using 64 threads\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 288 tensors from ./simpler-gemma-2-2b-multi/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = it-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type bf16:  183 tensors\n",
      "[   1/ 288]                    token_embd.weight - [ 2304, 256000,     1,     1], type =   bf16, converting to q8_0 .. size =  1125.00 MiB ->   597.66 MiB\n",
      "[   2/ 288]               blk.0.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   3/ 288]                blk.0.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[   4/ 288]                blk.0.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[   5/ 288]                  blk.0.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[   6/ 288]     blk.0.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   7/ 288]           blk.0.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   8/ 288]                blk.0.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   9/ 288]                  blk.0.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  10/ 288]             blk.0.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  11/ 288]                  blk.0.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  12/ 288]                  blk.0.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  13/ 288]               blk.1.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  14/ 288]                blk.1.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  15/ 288]                blk.1.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  16/ 288]                  blk.1.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  17/ 288]     blk.1.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  18/ 288]           blk.1.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  19/ 288]                blk.1.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  20/ 288]                  blk.1.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  21/ 288]             blk.1.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  22/ 288]                  blk.1.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  23/ 288]                  blk.1.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  24/ 288]              blk.10.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  25/ 288]               blk.10.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  26/ 288]               blk.10.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  27/ 288]                 blk.10.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  28/ 288]    blk.10.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  29/ 288]          blk.10.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  30/ 288]               blk.10.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  31/ 288]                 blk.10.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  32/ 288]            blk.10.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  33/ 288]                 blk.10.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  34/ 288]                 blk.10.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  35/ 288]              blk.11.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  36/ 288]               blk.11.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  37/ 288]               blk.11.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  38/ 288]                 blk.11.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  39/ 288]    blk.11.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  40/ 288]          blk.11.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  41/ 288]               blk.11.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  42/ 288]                 blk.11.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  43/ 288]            blk.11.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  44/ 288]                 blk.11.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  45/ 288]                 blk.11.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  46/ 288]              blk.12.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  47/ 288]               blk.12.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  48/ 288]               blk.12.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  49/ 288]                 blk.12.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  50/ 288]    blk.12.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  51/ 288]          blk.12.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  52/ 288]               blk.12.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  53/ 288]                 blk.12.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  54/ 288]            blk.12.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  55/ 288]                 blk.12.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  56/ 288]                 blk.12.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  57/ 288]              blk.13.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  58/ 288]               blk.13.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  59/ 288]               blk.13.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  60/ 288]                 blk.13.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  61/ 288]    blk.13.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  62/ 288]          blk.13.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  63/ 288]               blk.13.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  64/ 288]                 blk.13.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  65/ 288]            blk.13.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  66/ 288]                 blk.13.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  67/ 288]                 blk.13.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  68/ 288]              blk.14.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  69/ 288]               blk.14.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  70/ 288]               blk.14.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  71/ 288]                 blk.14.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  72/ 288]    blk.14.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  73/ 288]          blk.14.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  74/ 288]               blk.14.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  75/ 288]                 blk.14.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  76/ 288]            blk.14.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  77/ 288]                 blk.14.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  78/ 288]                 blk.14.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  79/ 288]              blk.15.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  80/ 288]               blk.15.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  81/ 288]               blk.15.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  82/ 288]                 blk.15.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  83/ 288]    blk.15.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  84/ 288]          blk.15.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  85/ 288]               blk.15.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  86/ 288]                 blk.15.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  87/ 288]            blk.15.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  88/ 288]                 blk.15.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  89/ 288]                 blk.15.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  90/ 288]              blk.16.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  91/ 288]               blk.16.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  92/ 288]               blk.16.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  93/ 288]                 blk.16.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[  94/ 288]    blk.16.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  95/ 288]          blk.16.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  96/ 288]               blk.16.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  97/ 288]                 blk.16.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  98/ 288]            blk.16.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[  99/ 288]                 blk.16.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 100/ 288]                 blk.16.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 101/ 288]              blk.17.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 102/ 288]               blk.17.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 103/ 288]               blk.17.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 104/ 288]                 blk.17.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 105/ 288]    blk.17.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 106/ 288]          blk.17.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 107/ 288]               blk.17.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 108/ 288]                 blk.17.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 109/ 288]            blk.17.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 110/ 288]                 blk.17.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 111/ 288]                 blk.17.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 112/ 288]              blk.18.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 113/ 288]               blk.18.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 114/ 288]               blk.18.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 115/ 288]                 blk.18.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 116/ 288]    blk.18.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 117/ 288]          blk.18.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 118/ 288]               blk.18.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 119/ 288]                 blk.18.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 120/ 288]            blk.18.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 121/ 288]                 blk.18.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 122/ 288]                 blk.18.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 123/ 288]              blk.19.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 124/ 288]               blk.19.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 125/ 288]               blk.19.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 126/ 288]                 blk.19.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 127/ 288]    blk.19.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 128/ 288]          blk.19.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 129/ 288]               blk.19.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 130/ 288]                 blk.19.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 131/ 288]            blk.19.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 132/ 288]                 blk.19.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 133/ 288]                 blk.19.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 134/ 288]               blk.2.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 135/ 288]                blk.2.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 136/ 288]                blk.2.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 137/ 288]                  blk.2.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 138/ 288]     blk.2.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 139/ 288]           blk.2.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 140/ 288]                blk.2.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 141/ 288]                  blk.2.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 142/ 288]             blk.2.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 143/ 288]                  blk.2.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 144/ 288]                  blk.2.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 145/ 288]              blk.20.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 146/ 288]               blk.20.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 147/ 288]               blk.20.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 148/ 288]                 blk.20.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 149/ 288]    blk.20.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 150/ 288]          blk.20.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 151/ 288]               blk.20.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 152/ 288]                 blk.20.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 153/ 288]            blk.20.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 154/ 288]                 blk.20.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 155/ 288]                 blk.20.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 156/ 288]              blk.21.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 157/ 288]               blk.21.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 158/ 288]               blk.21.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 159/ 288]                 blk.21.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 160/ 288]    blk.21.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 161/ 288]          blk.21.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 162/ 288]               blk.21.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 163/ 288]                 blk.21.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 164/ 288]            blk.21.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 165/ 288]                 blk.21.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 166/ 288]                 blk.21.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 167/ 288]              blk.22.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 168/ 288]               blk.22.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 169/ 288]               blk.22.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 170/ 288]                 blk.22.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 171/ 288]    blk.22.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 172/ 288]          blk.22.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 173/ 288]               blk.22.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 174/ 288]                 blk.22.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 175/ 288]            blk.22.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 176/ 288]                 blk.22.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 177/ 288]                 blk.22.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 178/ 288]              blk.23.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 179/ 288]               blk.23.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 180/ 288]               blk.23.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 181/ 288]                 blk.23.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 182/ 288]    blk.23.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 183/ 288]          blk.23.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 184/ 288]               blk.23.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 185/ 288]                 blk.23.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 186/ 288]            blk.23.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 187/ 288]                 blk.23.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 188/ 288]                 blk.23.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 189/ 288]               blk.24.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 190/ 288]                 blk.24.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 191/ 288]            blk.24.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 192/ 288]                 blk.24.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 193/ 288]                 blk.24.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 194/ 288]               blk.3.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 195/ 288]                blk.3.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 196/ 288]                blk.3.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 197/ 288]                  blk.3.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 198/ 288]     blk.3.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 199/ 288]           blk.3.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 200/ 288]                blk.3.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 201/ 288]                  blk.3.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 202/ 288]             blk.3.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 203/ 288]                  blk.3.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 204/ 288]                  blk.3.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 205/ 288]               blk.4.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 206/ 288]                blk.4.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 207/ 288]                blk.4.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 208/ 288]                  blk.4.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 209/ 288]     blk.4.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 210/ 288]           blk.4.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 211/ 288]                blk.4.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 212/ 288]                  blk.4.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 213/ 288]             blk.4.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 214/ 288]                  blk.4.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 215/ 288]                  blk.4.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 216/ 288]               blk.5.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 217/ 288]                blk.5.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 218/ 288]                blk.5.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 219/ 288]                  blk.5.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 220/ 288]     blk.5.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 221/ 288]           blk.5.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 222/ 288]                blk.5.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 223/ 288]                  blk.5.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 224/ 288]             blk.5.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 225/ 288]                  blk.5.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 226/ 288]                  blk.5.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 227/ 288]               blk.6.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 228/ 288]                blk.6.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 229/ 288]                blk.6.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 230/ 288]                  blk.6.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 231/ 288]     blk.6.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 232/ 288]           blk.6.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 233/ 288]                blk.6.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 234/ 288]                  blk.6.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 235/ 288]             blk.6.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 236/ 288]                  blk.6.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 237/ 288]                  blk.6.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 238/ 288]               blk.7.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 239/ 288]                blk.7.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 240/ 288]                blk.7.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 241/ 288]                  blk.7.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 242/ 288]     blk.7.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 243/ 288]           blk.7.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 244/ 288]                blk.7.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 245/ 288]                  blk.7.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 246/ 288]             blk.7.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 247/ 288]                  blk.7.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 248/ 288]                  blk.7.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 249/ 288]               blk.8.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 250/ 288]                blk.8.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 251/ 288]                blk.8.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 252/ 288]                  blk.8.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 253/ 288]     blk.8.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 254/ 288]           blk.8.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 255/ 288]                blk.8.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 256/ 288]                  blk.8.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 257/ 288]             blk.8.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 258/ 288]                  blk.8.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 259/ 288]                  blk.8.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 260/ 288]               blk.9.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 261/ 288]                blk.9.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 262/ 288]                blk.9.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 263/ 288]                  blk.9.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 264/ 288]     blk.9.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 265/ 288]           blk.9.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 266/ 288]                blk.9.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 267/ 288]                  blk.9.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 268/ 288]             blk.9.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 269/ 288]                  blk.9.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 270/ 288]                  blk.9.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 271/ 288]              blk.24.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 272/ 288]               blk.24.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 273/ 288]                 blk.24.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 274/ 288]    blk.24.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 275/ 288]          blk.24.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 276/ 288]               blk.24.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 277/ 288]              blk.25.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 278/ 288]               blk.25.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 279/ 288]               blk.25.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 280/ 288]                 blk.25.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q8_0 .. size =    40.50 MiB ->    21.52 MiB\n",
      "[ 281/ 288]    blk.25.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 282/ 288]          blk.25.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 283/ 288]               blk.25.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 284/ 288]                 blk.25.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 285/ 288]            blk.25.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 286/ 288]                 blk.25.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     9.00 MiB ->     4.78 MiB\n",
      "[ 287/ 288]                 blk.25.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 288/ 288]                   output_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "llama_model_quantize_internal: model size  =  4986.92 MB\n",
      "llama_model_quantize_internal: quant size  =  2649.74 MB\n",
      "\n",
      "main: quantize time =  3655.33 ms\n",
      "main:    total time =  3655.33 ms\n",
      "Unsloth: Conversion completed! Output location: ./simpler-gemma-2-2b-multi/unsloth.Q8_0.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\n",
      "main: build = 3509 (ecf6b7f2)\n",
      "main: built with x86_64-conda-linux-gnu-cc (conda-forge gcc 12.4.0-0) 12.4.0 for x86_64-conda-linux-gnu\n",
      "main: quantizing './simpler-gemma-2-2b-multi/unsloth.BF16.gguf' to './simpler-gemma-2-2b-multi/unsloth.Q5_K_M.gguf' as Q5_K_M using 64 threads\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 288 tensors from ./simpler-gemma-2-2b-multi/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = it-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type bf16:  183 tensors\n",
      "[   1/ 288]                    token_embd.weight - [ 2304, 256000,     1,     1], type =   bf16, converting to q6_K .. size =  1125.00 MiB ->   461.43 MiB\n",
      "[   2/ 288]               blk.0.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   3/ 288]                blk.0.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[   4/ 288]                blk.0.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[   5/ 288]                  blk.0.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[   6/ 288]     blk.0.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   7/ 288]           blk.0.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   8/ 288]                blk.0.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[   9/ 288]                  blk.0.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  10/ 288]             blk.0.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  11/ 288]                  blk.0.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  12/ 288]                  blk.0.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  13/ 288]               blk.1.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  14/ 288]                blk.1.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  15/ 288]                blk.1.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  16/ 288]                  blk.1.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  17/ 288]     blk.1.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  18/ 288]           blk.1.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  19/ 288]                blk.1.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  20/ 288]                  blk.1.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  21/ 288]             blk.1.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  22/ 288]                  blk.1.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  23/ 288]                  blk.1.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  24/ 288]              blk.10.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  25/ 288]               blk.10.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  26/ 288]               blk.10.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  27/ 288]                 blk.10.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  28/ 288]    blk.10.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  29/ 288]          blk.10.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  30/ 288]               blk.10.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  31/ 288]                 blk.10.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  32/ 288]            blk.10.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  33/ 288]                 blk.10.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  34/ 288]                 blk.10.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  35/ 288]              blk.11.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  36/ 288]               blk.11.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  37/ 288]               blk.11.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  38/ 288]                 blk.11.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  39/ 288]    blk.11.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  40/ 288]          blk.11.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  41/ 288]               blk.11.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  42/ 288]                 blk.11.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  43/ 288]            blk.11.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  44/ 288]                 blk.11.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  45/ 288]                 blk.11.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  46/ 288]              blk.12.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  47/ 288]               blk.12.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  48/ 288]               blk.12.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  49/ 288]                 blk.12.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  50/ 288]    blk.12.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  51/ 288]          blk.12.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  52/ 288]               blk.12.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  53/ 288]                 blk.12.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  54/ 288]            blk.12.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  55/ 288]                 blk.12.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  56/ 288]                 blk.12.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  57/ 288]              blk.13.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  58/ 288]               blk.13.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  59/ 288]               blk.13.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  60/ 288]                 blk.13.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  61/ 288]    blk.13.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  62/ 288]          blk.13.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  63/ 288]               blk.13.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  64/ 288]                 blk.13.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  65/ 288]            blk.13.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  66/ 288]                 blk.13.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  67/ 288]                 blk.13.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[  68/ 288]              blk.14.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  69/ 288]               blk.14.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  70/ 288]               blk.14.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  71/ 288]                 blk.14.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  72/ 288]    blk.14.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  73/ 288]          blk.14.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  74/ 288]               blk.14.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  75/ 288]                 blk.14.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  76/ 288]            blk.14.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  77/ 288]                 blk.14.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  78/ 288]                 blk.14.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  79/ 288]              blk.15.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  80/ 288]               blk.15.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  81/ 288]               blk.15.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  82/ 288]                 blk.15.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  83/ 288]    blk.15.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  84/ 288]          blk.15.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  85/ 288]               blk.15.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  86/ 288]                 blk.15.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  87/ 288]            blk.15.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  88/ 288]                 blk.15.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  89/ 288]                 blk.15.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  90/ 288]              blk.16.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  91/ 288]               blk.16.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[  92/ 288]               blk.16.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  93/ 288]                 blk.16.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[  94/ 288]    blk.16.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  95/ 288]          blk.16.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  96/ 288]               blk.16.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[  97/ 288]                 blk.16.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[  98/ 288]            blk.16.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[  99/ 288]                 blk.16.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 100/ 288]                 blk.16.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 101/ 288]              blk.17.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 102/ 288]               blk.17.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 103/ 288]               blk.17.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 104/ 288]                 blk.17.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 105/ 288]    blk.17.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 106/ 288]          blk.17.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 107/ 288]               blk.17.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 108/ 288]                 blk.17.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 109/ 288]            blk.17.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 110/ 288]                 blk.17.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 111/ 288]                 blk.17.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 112/ 288]              blk.18.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 113/ 288]               blk.18.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 114/ 288]               blk.18.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 115/ 288]                 blk.18.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 116/ 288]    blk.18.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 117/ 288]          blk.18.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 118/ 288]               blk.18.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 119/ 288]                 blk.18.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 120/ 288]            blk.18.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 121/ 288]                 blk.18.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 122/ 288]                 blk.18.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 123/ 288]              blk.19.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 124/ 288]               blk.19.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 125/ 288]               blk.19.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 126/ 288]                 blk.19.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 127/ 288]    blk.19.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 128/ 288]          blk.19.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 129/ 288]               blk.19.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 130/ 288]                 blk.19.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 131/ 288]            blk.19.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 132/ 288]                 blk.19.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 133/ 288]                 blk.19.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 134/ 288]               blk.2.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 135/ 288]                blk.2.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 136/ 288]                blk.2.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 137/ 288]                  blk.2.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 138/ 288]     blk.2.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 139/ 288]           blk.2.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 140/ 288]                blk.2.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 141/ 288]                  blk.2.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 142/ 288]             blk.2.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 143/ 288]                  blk.2.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 144/ 288]                  blk.2.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 145/ 288]              blk.20.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 146/ 288]               blk.20.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 147/ 288]               blk.20.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 148/ 288]                 blk.20.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 149/ 288]    blk.20.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 150/ 288]          blk.20.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 151/ 288]               blk.20.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 152/ 288]                 blk.20.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 153/ 288]            blk.20.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 154/ 288]                 blk.20.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 155/ 288]                 blk.20.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 156/ 288]              blk.21.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 157/ 288]               blk.21.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 158/ 288]               blk.21.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 159/ 288]                 blk.21.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 160/ 288]    blk.21.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 161/ 288]          blk.21.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 162/ 288]               blk.21.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 163/ 288]                 blk.21.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 164/ 288]            blk.21.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 165/ 288]                 blk.21.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 166/ 288]                 blk.21.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 167/ 288]              blk.22.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 168/ 288]               blk.22.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 169/ 288]               blk.22.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 170/ 288]                 blk.22.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 171/ 288]    blk.22.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 172/ 288]          blk.22.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 173/ 288]               blk.22.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 174/ 288]                 blk.22.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 175/ 288]            blk.22.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 176/ 288]                 blk.22.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 177/ 288]                 blk.22.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 178/ 288]              blk.23.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 179/ 288]               blk.23.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 180/ 288]               blk.23.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 181/ 288]                 blk.23.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 182/ 288]    blk.23.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 183/ 288]          blk.23.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 184/ 288]               blk.23.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 185/ 288]                 blk.23.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 186/ 288]            blk.23.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 187/ 288]                 blk.23.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 188/ 288]                 blk.23.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 189/ 288]               blk.24.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 190/ 288]                 blk.24.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 191/ 288]            blk.24.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 192/ 288]                 blk.24.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 193/ 288]                 blk.24.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 194/ 288]               blk.3.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 195/ 288]                blk.3.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 196/ 288]                blk.3.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 197/ 288]                  blk.3.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 198/ 288]     blk.3.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 199/ 288]           blk.3.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 200/ 288]                blk.3.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 201/ 288]                  blk.3.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 202/ 288]             blk.3.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 203/ 288]                  blk.3.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 204/ 288]                  blk.3.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 205/ 288]               blk.4.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 206/ 288]                blk.4.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 207/ 288]                blk.4.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 208/ 288]                  blk.4.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 209/ 288]     blk.4.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 210/ 288]           blk.4.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 211/ 288]                blk.4.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 212/ 288]                  blk.4.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 213/ 288]             blk.4.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 214/ 288]                  blk.4.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 215/ 288]                  blk.4.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 216/ 288]               blk.5.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 217/ 288]                blk.5.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 218/ 288]                blk.5.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 219/ 288]                  blk.5.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 220/ 288]     blk.5.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 221/ 288]           blk.5.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 222/ 288]                blk.5.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 223/ 288]                  blk.5.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 224/ 288]             blk.5.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 225/ 288]                  blk.5.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 226/ 288]                  blk.5.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 227/ 288]               blk.6.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 228/ 288]                blk.6.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 229/ 288]                blk.6.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 230/ 288]                  blk.6.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 231/ 288]     blk.6.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 232/ 288]           blk.6.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 233/ 288]                blk.6.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 234/ 288]                  blk.6.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 235/ 288]             blk.6.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 236/ 288]                  blk.6.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 237/ 288]                  blk.6.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 238/ 288]               blk.7.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 239/ 288]                blk.7.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 240/ 288]                blk.7.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 241/ 288]                  blk.7.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 242/ 288]     blk.7.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 243/ 288]           blk.7.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 244/ 288]                blk.7.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 245/ 288]                  blk.7.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 246/ 288]             blk.7.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 247/ 288]                  blk.7.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 248/ 288]                  blk.7.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 249/ 288]               blk.8.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 250/ 288]                blk.8.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 251/ 288]                blk.8.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 252/ 288]                  blk.8.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 253/ 288]     blk.8.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 254/ 288]           blk.8.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 255/ 288]                blk.8.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 256/ 288]                  blk.8.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 257/ 288]             blk.8.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 258/ 288]                  blk.8.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 259/ 288]                  blk.8.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 260/ 288]               blk.9.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 261/ 288]                blk.9.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 262/ 288]                blk.9.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 263/ 288]                  blk.9.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 264/ 288]     blk.9.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 265/ 288]           blk.9.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 266/ 288]                blk.9.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 267/ 288]                  blk.9.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 268/ 288]             blk.9.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 269/ 288]                  blk.9.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 270/ 288]                  blk.9.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 271/ 288]              blk.24.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 272/ 288]               blk.24.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 273/ 288]                 blk.24.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 274/ 288]    blk.24.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 275/ 288]          blk.24.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 276/ 288]               blk.24.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 277/ 288]              blk.25.attn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 278/ 288]               blk.25.ffn_down.weight - [ 9216,  2304,     1,     1], type =   bf16, converting to q6_K .. size =    40.50 MiB ->    16.61 MiB\n",
      "[ 279/ 288]               blk.25.ffn_gate.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 280/ 288]                 blk.25.ffn_up.weight - [ 2304,  9216,     1,     1], type =   bf16, converting to q5_K .. size =    40.50 MiB ->    13.92 MiB\n",
      "[ 281/ 288]    blk.25.post_attention_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 282/ 288]          blk.25.post_ffw_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 283/ 288]               blk.25.ffn_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "[ 284/ 288]                 blk.25.attn_k.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
      "[ 285/ 288]            blk.25.attn_output.weight - [ 2048,  2304,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 286/ 288]                 blk.25.attn_q.weight - [ 2304,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     9.00 MiB ->     3.09 MiB\n",
      "[ 287/ 288]                 blk.25.attn_v.weight - [ 2304,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n",
      "[ 288/ 288]                   output_norm.weight - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
      "llama_model_quantize_internal: model size  =  4986.92 MB\n",
      "llama_model_quantize_internal: quant size  =  1828.42 MB\n",
      "\n",
      "main: quantize time =  7423.51 ms\n",
      "main:    total time =  7423.51 ms\n",
      "Unsloth: Conversion completed! Output location: ./simpler-gemma-2-2b-multi/unsloth.Q5_K_M.gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7a4bc321864d0e9d35bba4c26bf032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.BF16.gguf:   0%|          | 0.00/5.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/davidbzyk/simpler-gemma-2-2b-multi\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a8ce975cf64908ab2143f5740052e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q4_K_M.gguf:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/davidbzyk/simpler-gemma-2-2b-multi\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c510ca85a44e73bf3fd577b650f2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q8_0.gguf:   0%|          | 0.00/2.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/davidbzyk/simpler-gemma-2-2b-multi\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72ea3ea547b4ad28ec4ccd1ffcb6b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q5_K_M.gguf:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/davidbzyk/simpler-gemma-2-2b-multi\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    model.push_to_hub_gguf(\n",
    "        multi_models_name, # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token=HUGGING_FACE_HUB_TOKEN, # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"lora_model\") # Local saving\n",
    "#tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to save individually core models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
